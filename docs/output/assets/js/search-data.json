{"0": {
    "doc": "Authentication",
    "title": "User Authentication",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/auth.html#user-authentication",
    "relUrl": "/admin/auth.html#user-authentication"
  },"1": {
    "doc": "Authentication",
    "title": "Internal (Database)",
    "content": "Out of the box, only one user is created during installation. It is the admin account. This account is always persisted in the database no matter what. You can also create internal users when needed. See Create User . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/auth.html#internal-database",
    "relUrl": "/admin/auth.html#internal-database"
  },"2": {
    "doc": "Authentication",
    "title": "External",
    "content": "When an external authentication mechanism is used, the users are automatically synced when they first connect to the platform. Cognito (Amazon) . To use Cognito with Elastic bean stalk, you need to configure https first: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/configuring-https.html . Activate Cognito in your AWS tenant: . | https://www.baeldung.com/spring-security-oauth-cognito | https://docs.aws.amazon.com/cognito/latest/developerguide/getting-started-with-cognito-user-pools.html | . Activate the amazon profile: . | Modify the service.conf file (in /opt/datamaker) JAVA_OPTS='-Xmx512M -Dspring.profiles.active=amazon,cognito -Dlogging.pattern.console= -Dlogging.file.path=/opt/datamaker/logs' . | . You can also use an environment variable: export spring_profiles_active=cognito . Create application-amazon.properties inside ${application.config.path}/jar directory. Make sure to select the jar file type. You can use the file manager to do that: Create resource file . Fill out the clientId, clientSecret, server, issuerUri (if needed) . spring: security: oauth2: client: registration: cognito: clientId: ... clientSecret: .... scope: openid redirect-uri: http://{server}/datamaker/login/oauth2/code/cognito clientName: datamaker-client provider: cognito: issuerUri: https://cognito-idp.ca-central-1.amazonaws.com/ca-central-1_zlOxEgek0 user-name-attribute: cognito:username . Azure AD . Activate OAuth in your Azure tenant: . | https://docs.microsoft.com/en-ca/azure/active-directory/develop/quickstart-register-app | https://docs.microsoft.com/en-us/azure/developer/java/spring-framework/configure-spring-boot-starter-java-app-with-azure-active-directory | https://github.com/Azure/azure-sdk-for-java/tree/master/sdk/spring/azure-spring-boot-samples/azure-spring-boot-sample-active-directory-backend | . Activate the azure profile: . | Modify the service.conf file (in /opt/datamaker) JAVA_OPTS='-Xmx512M -Dspring.profiles.active=azure -Dlogging.pattern.console= -Dlogging.file.path=/opt/datamaker/logs' . | . You can also use an environment variable: export spring_profiles_active=azure . Create application-azure.yml inside ${application.config.path}/jar directory. Make sure to select the jar file type. You can use the file manager to do that: Create resource file . Fill out the client-id, client-secret, server, tenant-id, allowed-groups, admin.roles, user.roles . azure: activedirectory: tenant-id: 3-312-312-312-312 client-id: 213-312-312-86a8-3212 client-secret: 3212321231 user-group: allowed-group-names: group1, group2 admn: roles: group1 user: roles: group2 . Ldap (generic) . Activate the ldap profile: . | Modify the service.conf file (in /opt/datamaker) JAVA_OPTS='-Xmx512M -Dspring.profiles.active=ldap -Dlogging.pattern.console= -Dlogging.file.path=/opt/datamaker/logs' . | . You can also use an environment variable: export spring_profiles_active=ldap . Create application-azure.properties inside ${application.config.path}/jar directory. Make sure to select the jar file type. You can use the file manager to do that: Create resource file . Fill out the userSearchFilter, url, domain, rootDn, OU . # LDAP AUTHENTICATION MANAGER security.ldap.userSearchFilter=userPrincipalName={0} security.ldap.url=ldap://azure.datamaker.ai:389/ security.ldap.domain=datamaker.ai security.ldap.rootDn=DC=azure,DC=datamaker,DC=ai OU=AADDC%20Users,DC=azure,DC=datamaker,DC=ai . Active directory . Activate the ldap-ad profile: . | Modify the service.conf file (in /opt/datamaker) JAVA_OPTS='-Xmx512M -Dspring.profiles.active=ldap-ad -Dlogging.pattern.console= -Dlogging.file.path=/opt/datamaker/logs' . | . You can also use an environment variable: export spring_profiles_active=ldap . Create application-ldap-ad.properties inside ${application.config.path}/jar directory. Make sure to select the jar file type. You can use the file manager to do that: Create resource file . Fill out the required properties. # AUTHORITIES admin.roles=MANAGERS user.roles=DEVELOPERS # LDAP AUTHENTICATION MANAGER security.ldap.userDnPatterns=\"uid={0},ou=people\" security.ldap.userSearchBase= security.ldap.userSearchFilter= security.ldap.groupSearchBase=\"ou=groups\" security.ldap.groupSearchFilter=(uniqueMember={0}) security.ldap.url=ldap://localhost:8389/dc=springframework,dc=org security.ldap.passwordAttribute=userPassword security.ldap.managerDn= security.ldap.managerPassword= . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/auth.html#external",
    "relUrl": "/admin/auth.html#external"
  },"3": {
    "doc": "Authentication",
    "title": "Google",
    "content": "See OAuth2 . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/auth.html#google",
    "relUrl": "/admin/auth.html#google"
  },"4": {
    "doc": "Authentication",
    "title": "Authentication",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/auth.html",
    "relUrl": "/admin/auth.html"
  },"5": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/configuration.html",
    "relUrl": "/admin/configuration.html"
  },"6": {
    "doc": "Configuration",
    "title": "Properties file",
    "content": "The application settings are stored in /opt/datamaker/application.properties file. You can edit them and restart the application for taking effect. ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/configuration.html#properties-file",
    "relUrl": "/admin/configuration.html#properties-file"
  },"7": {
    "doc": "Configuration",
    "title": "Java System properties",
    "content": "You can override application settings from the JAVA_OPTS variable in service.conf (or in Tomcat setenv.sh). JAVA_OPTS=’-Xmx512M -Dlogging.config=/opt/datamaker/logback-spring.xml’ . Example: -logging.file.max-size=50MB . JAVA_OPTS=’-Xmx512M -Dlogging.config=/opt/datamaker/logback-spring.xml -logging.file.max-size=50MB’ . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/configuration.html#java-system-properties",
    "relUrl": "/admin/configuration.html#java-system-properties"
  },"8": {
    "doc": "Configuration",
    "title": "Binding from Environment Variables",
    "content": "You can also use Windows or Linux environment variable to override application settings. You need to use the snake case formatting. Ex: spring.datasource.url becomes SPRING_DATASOURCE_URL . EXPORT SPRING_DATASOURCE_URL=jdbc:mysql://127.0.0.1:3306/datamaker . Naming convention . Most operating systems impose strict rules around the names that can be used for environment variables. For example, Linux shell variables can contain only letters (a to z or A to Z), numbers (0 to 9) or the underscore character (_). By convention, Unix shell variables will also have their names in UPPERCASE. Spring Boot’s relaxed binding rules are, as much as possible, designed to be compatible with these naming restrictions. To convert a property name in the canonical-form to an environment variable name you can follow these rules: . Replace dots (.) with underscores (_). Remove any dashes (-). Convert to uppercase. For example, the configuration property spring.main.log-startup-info would be an environment variable named SPRING_MAIN_LOGSTARTUPINFO. Environment variables can also be used when binding to object lists. To bind to a List, the element number should be surrounded with underscores in the variable name. For example, the configuration property my.service[0].other would use an environment variable named MY_SERVICE_0_OTHER. ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/configuration.html#binding-from-environment-variables",
    "relUrl": "/admin/configuration.html#binding-from-environment-variables"
  },"9": {
    "doc": "Database",
    "title": "Database setup",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/database.html#database-setup",
    "relUrl": "/admin/database.html#database-setup"
  },"10": {
    "doc": "Database",
    "title": "Install Mysql",
    "content": "https://dev.mysql.com/doc/mysql-installation-excerpt/8.0/en/ . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/database.html#install-mysql",
    "relUrl": "/admin/database.html#install-mysql"
  },"11": {
    "doc": "Database",
    "title": "Database creation",
    "content": "CREATE DATABASE datamaker; . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/database.html#database-creation",
    "relUrl": "/admin/database.html#database-creation"
  },"12": {
    "doc": "Database",
    "title": "Schema migration",
    "content": "Database schema upgrades are manage automatically by the application. ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/database.html#schema-migration",
    "relUrl": "/admin/database.html#schema-migration"
  },"13": {
    "doc": "Database",
    "title": "Database backup",
    "content": "https://dev.mysql.com/doc/refman/8.0/en/backup-and-recovery.html . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/database.html#database-backup",
    "relUrl": "/admin/database.html#database-backup"
  },"14": {
    "doc": "Database",
    "title": "Database",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/database.html",
    "relUrl": "/admin/database.html"
  },"15": {
    "doc": "Datasets",
    "title": "Datasets",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/datasets.html",
    "relUrl": "/user/datasets.html"
  },"16": {
    "doc": "Datasets",
    "title": "Table of contents",
    "content": ". | To view a list of all datasets: Datasets | Manual creation . | To create a new dataset: Create Dataset | . | Infer dataset (auto-detect) . | To create a dataset based on a sample file: InferDataset | . | Content types . | Avro | Csv | Excel | Json | Json Schema | Parquet | Sql | Xml | Xml Schema | . | . A dataset is the main component of data simulation. It can represent any kind of data (file, table, topic). To ease understanding, you can image a dataset as a table of a database. A table columns with types. A dataset contains different kind of fields. To view a list of all datasets: Datasets . From here you can see the list of all datasets available to your user. You can create a new dataset or infer one using the buttons at the top right section of the page. From the main results section, you can modify the fields, the dataset or delete it. ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/datasets.html#table-of-contents",
    "relUrl": "/user/datasets.html#table-of-contents"
  },"17": {
    "doc": "Datasets",
    "title": "Manual creation",
    "content": "To create a new dataset: Create Dataset . | Name | Workspace: Select the workspace where the dataset will be located | Description | Locale: Will be use as the default locale for data generation (see Fields) | Tags: Use for helping search index | Export header: Flag that determines if headers will be exposed in file (example: first line of CSV) | Nullable percent: How many records (in %) that can contains null value | Allow duplicate values: Determines if generated dataset can contains duplicate records | Duplicate percent limit: % of acceptable duplicate records over all the records | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/datasets.html#manual-creation",
    "relUrl": "/user/datasets.html#manual-creation"
  },"18": {
    "doc": "Datasets",
    "title": "Infer dataset (auto-detect)",
    "content": "To create a dataset based on a sample file: InferDataset . | Name | Workspace: Select the workspace where the dataset will be located | Content sample: Paste an example of the records (or upload file) | File custom input: File containing some samples | Select content type: Refer to the Content Types section to see which settings are needed to process the associated file | Supported file type: . | AVRO | CSV | EXCEL | JSON | JSON Schema | PARQUET | TEXT | SQL | XML | XSD | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/datasets.html#infer-dataset-auto-detect",
    "relUrl": "/user/datasets.html#infer-dataset-auto-detect"
  },"19": {
    "doc": "Datasets",
    "title": "Content types",
    "content": "Avro . Description: . Class: ca.breakpoints.datamaker.processor.AvroProcessor . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | . Csv . Description: . Class: ca.breakpoints.datamaker.processor.CsvProcessor . Configuration: . | Number of lines to skip . | Type: NUMERIC | Default value: 0 | Possible values: | . | Number of lines to process . | Type: NUMERIC | Default value: 10 | Possible values: | . | File encoding . | Type: STRING | Default value: UTF-8 | Possible values: | . | File encoding . | Type: STRING | Default value: , | Possible values: | . | Treat first line as header . | Type: BOOLEAN | Default value: True | Possible values: True, False | . | Input filename . | Type: STRING | Default value: | Possible values: | . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | . Excel . Description: . Class: ca.breakpoints.datamaker.processor.ExcelProcessor . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | Sheet name . | Type: STRING | Default value: | Possible values: | . | Sheet number . | Type: NUMERIC | Default value: 0 | Possible values: | . | Process rows . | Type: BOOLEAN | Default value: True | Possible values: | . | Columns to skip . | Type: LIST | Default value: [] | Possible values: | . | Which row number to use as header . | Type: NUMERIC | Default value: | Possible values: | . | Which row number to use as datatype . | Type: NUMERIC | Default value: | Possible values: | . | Number of rows to skip . | Type: NUMERIC | Default value: 0 | Possible values: | . | Number of lines to process . | Type: NUMERIC | Default value: 10 | Possible values: | . | Which column number to use as header . | Type: NUMERIC | Default value: | Possible values: | . | Which column number to use as datatype . | Type: NUMERIC | Default value: | Possible values: | . | Which column number to use as comment/description . | Type: NUMERIC | Default value: 0 | Possible values: | . | . Json . Description: . Class: ca.breakpoints.datamaker.processor.JsonProcessor . Configuration: . | Input filename . | Type: STRING | Default value: | Possible values: | . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | . Json Schema . Description: . Class: ca.breakpoints.datamaker.processor.JsonSchemaProcessor . Configuration: . Parquet . Description: . Class: ca.breakpoints.datamaker.processor.ParquetProcessor . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | . Sql . Description: . Class: ca.breakpoints.datamaker.processor.SqlProcessor . Configuration: . | Input filename . | Type: STRING | Default value: | Possible values: | . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | . Xml . Description: . Class: ca.breakpoints.datamaker.processor.XmlProcessor . Configuration: . | Input filename . | Type: STRING | Default value: | Possible values: | . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | XML Root element . | Type: STRING | Default value: | Possible values: | . | Number of lines to process . | Type: NUMERIC | Default value: 10 | Possible values: | . | . Xml Schema . Description: . Class: ca.breakpoints.datamaker.processor.XmlSchemaProcessor . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | Input filename . | Type: STRING | Default value: | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/datasets.html#content-types",
    "relUrl": "/user/datasets.html#content-types"
  },"20": {
    "doc": "Expression Language",
    "title": "Expression Language",
    "content": "The Spring Expression Language (“SpEL” for short) is a powerful expression language that supports querying and manipulating an object graph at runtime. The language syntax is similar to Unified EL but offers additional features, most notably method invocation and basic string templating functionality. Reference . The expression language supports the following functionality: . | Literal expressions | Boolean and relational operators | Regular expressions | Class expressions | Accessing properties, arrays, lists, and maps | Method invocation | Relational operators | Assignment | Calling constructors | Bean references | Array construction | Inline lists | Inline maps | Ternary operator | Variables | User-defined functions | Collection projection | Collection selection | Templated expressions | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/expression_language.html",
    "relUrl": "/user/expression_language.html"
  },"21": {
    "doc": "Expression Language",
    "title": "Available variables and fields",
    "content": ". | dataset (Dataset) . | dateCreated (Date) | dateModified (Date) | description (String) | externalId (UUID) | id (Long) | locale (Locale) | name (String) | tags (Set&lt;String&gt;) | . | dataJob (GenerateDataJob) . | config (HashMap) | dateCreated (Date) | dateModified (Date) | description (String) | externalId (UUID) | id (Long) | locale (Locale) | generator (DataGenerator) | dataType . | name (String) | . | name (String) | schedule (String) | sinkNames (Set&lt;String&gt;) | tags (Set&lt;String&gt;) | . | jobExecution . | cancelTime (Date) | dataJob (GenerateDataJob) | endTime (Date) | errors (List&lt;String&gt;) | externalId (UUID) | id (Long) | numberOfRecords | results (List&lt;String&gt;) | startTime (Date) | state (String) | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/expression_language.html#available-variables-and-fields",
    "relUrl": "/user/expression_language.html#available-variables-and-fields"
  },"22": {
    "doc": "Expression Language",
    "title": "Examples",
    "content": "#dataset.name + \"-\" + T(java.lang.System).currentTimeMillis() + \".\" + #dataJob.generator.dataType.name().toLowerCase() . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/expression_language.html#examples",
    "relUrl": "/user/expression_language.html#examples"
  },"23": {
    "doc": "Field Mappings",
    "title": "Field Mappings",
    "content": "Field mapping is used for mapping properties found in sample files and their corresponding field representation. FieldMappings . For instance, we have a mapping for Address to AddressField. This configuration is used for dataset inference (InferDataset). When sample files are processes (ex: CSV), each header or properties will be compared to field mappings. If there is a positive match (ex: an address property is found), a field will be automatically created using the mapping configuration found. This improves the quality of generated data. Instead of matching on data type only, we match the most relevant field. The data will be more accurate if we are able to match out of the box. The fallback mechanism of inference is on the data type. You can also specify for which language the mapping applies to. Create field mappings . | Name | Language: Choose a locale | Primary key: Use this option if the field will be referenced by other fields (behave like a foreign key) | Nullable: Is the field nullable | Data type: Select a field type | Formatter: Select a data formatter | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/field_mappings.html",
    "relUrl": "/user/field_mappings.html"
  },"24": {
    "doc": "Fields",
    "title": "Fields",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html",
    "relUrl": "/user/fields.html"
  },"25": {
    "doc": "Fields",
    "title": "Table of contents",
    "content": ". | Add a new field to a dataset | Supported Fields . | BUSINESS . | Company | . | CUSTOM . | Choice list | Nested field (map) | Constant value | Crypto Field | Custom string | Empty value | File | Finance | Item identification number | Multimedia Field | Reference | Regex expression | Script Field | Sequence | Sgml Field | Text | Uuid Field | . | IDENTITY . | Age | Demographic | Email address | Name | Phone number | Social Network Field | Social Number Field | Url Field | . | JOB . | Job | . | MONEY . | Credit Card Field | Money | . | MULTIMEDIA . | Image Field | . | NETWORK . | Network related | Password | Username Field | . | PHYSICAL_LOCATION . | Address | GeoJSON | . | PRIMITIVE . | List of other fields | Big Integer Field | Boolean value | Bytes Field | Date Time | Big decimal | Floating point: double precision | Duration Field | Floating point: simple precision | Integer Field | Long | Null value | Period Field | Stock Field | String Field | Timestamp Field | . | . | . Add a new field to a dataset . Most field are terminal, meaning that they cannot contain other fields. If you want to represent an array or list, you can use ArrayField. The array field container, can only contain one type of field. You can have a list of DoubleField, but you cannot have a list containing different elements. If you need to include different elements, you can use ComplexField. It behaves like a Map. Each field is associated with a key. It can be used to represent a JSON file. If you need to include a field from another dataset, you can use ReferenceField. It can emulate the behavior of primary key &lt;-&gt; foreign key you find in databases. For example: . | Table A |   | . | LongField | id | . | StringField | name | . | Table B |   | . | ReferenceField | TableA:id | . | SalaryField | wages | . | Name | Data Type: . | See Supported fields | . | Formatter: . | See Formatter section | . | Locale | Primary | Nullable | Actions . | Alias: Copy the field definition for later reuse | Edit: Additional settings | Move Up | Move Down | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#table-of-contents",
    "relUrl": "/user/fields.html#table-of-contents"
  },"26": {
    "doc": "Fields",
    "title": "Supported Fields",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#supported-fields",
    "relUrl": "/user/fields.html#supported-fields"
  },"27": {
    "doc": "Fields",
    "title": "BUSINESS",
    "content": "Company . Description: Business/legal entity/company name . Class: ca.breakpoints.datamaker.model.field.type.CompanyField . Configuration: . | Company type . | Type: STRING | Default value: NAME | Possible values: NAME, BUZZWORD, CATCH_PHRASE, PROFESSION, LOGO, INDUSTRY, URL, SUFFIX | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#business",
    "relUrl": "/user/fields.html#business"
  },"28": {
    "doc": "Fields",
    "title": "CUSTOM",
    "content": "Choice list . Description: List of pre-defined values . Class: ca.breakpoints.datamaker.model.field.type.ChoiceField . Configuration: . | Value type . | Type: STRING | Default value: STRING | Possible values: NUMERIC, STRING | . | Choices values . | Type: LIST | Default value: [] | Possible values: | . | . Nested field (map) . Description: Will contains other fields . Class: ca.breakpoints.datamaker.model.field.type.ComplexField . Configuration: . | Element values . | Type: FIELDS | Default value: [] | Possible values: | . | . Constant value . Description: Value that never changes . Class: ca.breakpoints.datamaker.model.field.type.ConstantField . Configuration: . | Constant value . | Type: STRING | Default value: | Possible values: | . | Value type . | Type: STRING | Default value: STRING | Possible values: BOOLEAN, DATE, NUMERIC, STRING | . | . Crypto Field . Description: Hashed value: MD5, SHA-256 . Class: ca.breakpoints.datamaker.model.field.type.CryptoField . Configuration: . | Algorithm . | Type: STRING | Default value: SHA256 | Possible values: BCRYPT, MD5, SHA1, SHA256, SHA512, SIMPLE | . | . Custom string . Description: . Returns a string with the '#' characters in the parameter replaced with random digits between 0-9 inclusive. Returns a string with the '?' characters in the parameter replaced with random alphabetic characters. Example: FLIGHT-???-#### will generate a value of FLIGHT-ZUK-8943 . Class: ca.breakpoints.datamaker.model.field.type.CustomField . Configuration: . | Pattern . | Type: STRING | Default value: | Possible values: | . | Uppercase value . | Type: BOOLEAN | Default value: False | Possible values: | . | . Empty value . Description: Empty value field . Class: ca.breakpoints.datamaker.model.field.type.EmptyField . Configuration: . | Empty value . | Type: STRING | Default value: | Possible values: | . | . File . Description: Filename . Class: ca.breakpoints.datamaker.model.field.type.FileField . Configuration: . | File data type . | Type: STRING | Default value: NAME | Possible values: NAME, EXTENSION, MIME_TYPE | . | Directory name . | Type: STRING | Default value: None | Possible values: | . | File name . | Type: STRING | Default value: None | Possible values: | . | Path separator . | Type: STRING | Default value: None | Possible values: | . | File extension . | Type: STRING | Default value: None | Possible values: | . | . Finance . Description: Financial . Class: ca.breakpoints.datamaker.model.field.type.FinanceField . Configuration: . | Finance type . | Type: STRING | Default value: BIC | Possible values: VAT_NUMBER, IBAN, CC, BIC, ACCOUNT_NUMBER, TAX_NUMBER | . | . Item identification number . Description: Item identification number . Class: ca.breakpoints.datamaker.model.field.type.IdentificationNumberField . Configuration: . | Identification number type . | Type: STRING | Default value: UPC | Possible values: BARCODE, UPC, ISBN, ASIN, ISBN10, ISBN13, IMEI, EAN8, EAN13, GTIN8, GTIN13 | . | . Multimedia Field . Description: Multimedia content . Class: ca.breakpoints.datamaker.model.field.type.MultimediaField . Configuration: . Reference . Description: A reference to another field in the current dataset . Class: ca.breakpoints.datamaker.model.field.type.ReferenceField . Configuration: . | Reference value . | Type: REFERENCE | Default value: None | Possible values: | . | . Regex expression . Description: Regular expression . Class: ca.breakpoints.datamaker.model.field.type.RegexField . Configuration: . | Pattern . | Type: STRING | Default value: \\w{10} | Possible values: | . | . Script Field . Description: Scripting (Uses SpEL expression language) . Class: ca.breakpoints.datamaker.model.field.type.ScriptField . Configuration: . | Script . | Type: STRING | Default value: T(java.lang.Math).random() * 100.0 | Possible values: | . | Script variable names . | Type: LIST | Default value: [] | Possible values: | . | Script variable values . | Type: LIST | Default value: [] | Possible values: | . | . Sequence . Description: Primary key . Class: ca.breakpoints.datamaker.model.field.type.SequenceField . Configuration: . | Minimum value . | Type: NUMERIC | Default value: 1 | Possible values: | . | Initial value . | Type: NUMERIC | Default value: 0 | Possible values: | . | . Sgml Field . Description: HTML/XML documents . Class: ca.breakpoints.datamaker.model.field.type.SgmlField . Configuration: . | SGL Document type . | Type: STRING | Default value: HTML | Possible values: HTML, XML | . | . Text . Description: Generated text (locale based dictionary) . Class: ca.breakpoints.datamaker.model.field.type.TextField . Configuration: . | Text type . | Type: STRING | Default value: WORD | Possible values: WORD, WORDS, SENTENCES, PARAGRAPHS | . | Length . | Type: NUMERIC | Default value: 25 | Possible values: | . | Exclude stop words . | Type: BOOLEAN | Default value: False | Possible values: | . | . Uuid Field . Description: Immutable universally unique identifier (UUID) . Class: ca.breakpoints.datamaker.model.field.type.UuidField . Configuration: . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#custom",
    "relUrl": "/user/fields.html#custom"
  },"29": {
    "doc": "Fields",
    "title": "IDENTITY",
    "content": "Age . Description: Age, Integer [1-100] . Class: ca.breakpoints.datamaker.model.field.type.AgeField . Configuration: . | Mininum age . | Type: NUMERIC | Default value: 1 | Possible values: 1, 125 | . | Maximum age . | Type: NUMERIC | Default value: 125 | Possible values: 1, 125 | . | . Demographic . Description: Demographic . Class: ca.breakpoints.datamaker.model.field.type.DemographicField . Configuration: . | Demographic type . | Type: STRING | Default value: GENDER | Possible values: MARITAL_STATUS, GENDER, RACE, EDUCATION, DEMONYM | . | . Email address . Description: Email address . Class: ca.breakpoints.datamaker.model.field.type.EmailField . Configuration: . Name . Description: Full name . Class: ca.breakpoints.datamaker.model.field.type.NameField . Configuration: . | Name type . | Type: STRING | Default value: FULL | Possible values: FULL, FIRST, LAST, MIDDLE, TITLE, PREFIX, SUFFIX | . | . Phone number . Description: Home phone . Class: ca.breakpoints.datamaker.model.field.type.PhoneField . Configuration: . | Name type . | Type: STRING | Default value: HOME | Possible values: MOBILE, HOME, BUSINESS, FAX, EXTENSION | . | . Social Network Field . Description: Social network . Class: ca.breakpoints.datamaker.model.field.type.SocialNetworkField . Configuration: . | Social network type . | Type: STRING | Default value: PROFILE_NAME | Possible values: PROFILE_PICTURE, SOCIAL_HANDLE, PROFILE_NAME | . | . Social Number Field . Description: Social security number . Class: ca.breakpoints.datamaker.model.field.type.SocialNumberField . Configuration: . Url Field . Description: Generic URL address . Class: ca.breakpoints.datamaker.model.field.type.UrlField . Configuration: . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#identity",
    "relUrl": "/user/fields.html#identity"
  },"30": {
    "doc": "Fields",
    "title": "JOB",
    "content": "Job . Description: Job position . Class: ca.breakpoints.datamaker.model.field.type.JobField . Configuration: . | Job type . | Type: STRING | Default value: TITLE | Possible values: TITLE, SENIORITY, KEY_SKILLS, POSITION, FIELD | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#job",
    "relUrl": "/user/fields.html#job"
  },"31": {
    "doc": "Fields",
    "title": "MONEY",
    "content": "Credit Card Field . Description: Credit card number . Class: ca.breakpoints.datamaker.model.field.type.CreditCardField . Configuration: . | Credit card type . | Type: STRING | Default value: RANDOM | Possible values: RANDOM, VISA, AMERICAN_EXPRESS, DANKORT, DINERS_CLUB, DISCOVER, FORBRUGSFORENINGEN, JCB, LASER, MASTERCARD, SOLO, SWITCH | . | . Money . Description: Money/any currency . Class: ca.breakpoints.datamaker.model.field.type.MoneyField . Configuration: . | Minimum value . | Type: NUMERIC | Default value: 0.0 | Possible values: -9223372036854775808, 0 | . | Maximum value . | Type: NUMERIC | Default value: 99999.99 | Possible values: 0, 9223372036854775807 | . | Negative value . | Type: BOOLEAN | Default value: False | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#money",
    "relUrl": "/user/fields.html#money"
  },"32": {
    "doc": "Fields",
    "title": "MULTIMEDIA",
    "content": "Image Field . Description: Image content . Class: ca.breakpoints.datamaker.model.field.type.ImageField . Configuration: . | Image format type . | Type: STRING | Default value: JPEG | Possible values: JPEG, PNG, GIF, BMP, WBMP, TIF, TIFF | . | Image width . | Type: NUMERIC | Default value: 640 | Possible values: | . | Image height . | Type: NUMERIC | Default value: 480 | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#multimedia",
    "relUrl": "/user/fields.html#multimedia"
  },"33": {
    "doc": "Fields",
    "title": "NETWORK",
    "content": "Network related . Description: IPv4/IPv6/MAC address . Class: ca.breakpoints.datamaker.model.field.type.NetworkField . Configuration: . | Network address type . | Type: STRING | Default value: IPv4 | Possible values: MAC, IPv4, IPv6, HOSTNAME, IPv4_CIDR, IPv6_CIDR | . | Use private range only . | Type: BOOLEAN | Default value: False | Possible values: | . | . Password . Description: Password . Class: ca.breakpoints.datamaker.model.field.type.PasswordField . Configuration: . | Minimum password length . | Type: NUMERIC | Default value: 8 | Possible values: 0, 10000 | . | Maximum password length . | Type: NUMERIC | Default value: 16 | Possible values: 0, 10000 | . | Include digits . | Type: BOOLEAN | Default value: True | Possible values: | . | Include special characters . | Type: BOOLEAN | Default value: False | Possible values: | . | Include upper case . | Type: BOOLEAN | Default value: False | Possible values: | . | . Username Field . Description: Username . Class: ca.breakpoints.datamaker.model.field.type.UsernameField . Configuration: . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#network",
    "relUrl": "/user/fields.html#network"
  },"34": {
    "doc": "Fields",
    "title": "PHYSICAL_LOCATION",
    "content": "Address . Description: Fully formatted address using locale . Class: ca.breakpoints.datamaker.model.field.type.AddressField . Configuration: . | Address type . | Type: STRING | Default value: FULL | Possible values: FULL, STREET_NAME, STREET_WITH_NUMBER, ZIP_CODE, CEDEX, POSTAL_CODE, COUNTRY, STATE, PROVINCE, CITY | . | . GeoJSON . Description: . Represents a GeoJSON coordinates. Degrees of latitude are parallel so the distance between each degree remains almost constant but since degrees of longitude are farthest apart at the equator and converge at the poles, their distance varies greatly. Each degree of latitude is approximately 69 miles (111 kilometers) apart. The range varies (due to the earth's slightly ellipsoid shape) from 68.703 miles (110.567 km) at the equator to 69.407 (111.699 km) at the poles. This is convenient because each minute (1/60th of a degree) is approximately one [nautical] mile. A degree of longitude is widest at the equator at 69.172 miles (111.321) and gradually shrinks to zero at the poles. At 40° north or south the distance between a degree of longitude is 53 miles (85 km) What is the \"right-hand-rule\"? When you construct a polygon, you can order the coordinates in one direction or another. If you’re drawing a circle, you might start on the left and go counter-clockwise around to meet the original point. Or, you might go clockwise. Here is the specification: A linear ring MUST follow the right-hand rule with respect to the area it bounds, i.e., exterior rings are counterclockwise, and holes are clockwise. Class: ca.breakpoints.datamaker.model.field.type.GeographicField . Configuration: . | Geographic type . | Type: STRING | Default value: POINT | Possible values: LATITUDE, LONGITUDE, COORDINATES, POINT, LINE_STRING, MULTI_LINE_STRING, POLYGON, MULTI_POLYGON, FEATURE, FEATURE_COLLECTION | . | Object size . | Type: NUMERIC | Default value: 10 | Possible values: 0, 32767 | . | Steps delta factor . | Type: NUMERIC | Default value: 1.0 | Possible values: 0, 3.4028235e+38 | . | Country bounding box . | Type: STRING | Default value: None | Possible values: Afghanistan, Angola, Albania, United Arab Emirates, Argentina, Armenia, Antarctica, Fr. S. and Antarctic Lands, Australia, Austria, Azerbaijan, Burundi, Belgium, Benin, Burkina Faso, Bangladesh, Bulgaria, Bahamas, Bosnia and Herz., Belarus, Belize, Bolivia, Brazil, Brunei, Bhutan, Botswana, Central African Rep., Canada, Switzerland, Chile, China, Ivory Coast, Cameroon, Congo (Kinshasa), Congo (Brazzaville), Colombia, Costa Rica, Cuba, Cyprus, Czech Rep., Germany, Djibouti, Denmark, Dominican Rep., Algeria, Ecuador, Egypt, Eritrea, Spain, Estonia, Ethiopia, Finland, Fiji, Falkland Is., France, Gabon, United Kingdom, Georgia, Ghana, Guinea, Gambia, Guinea Bissau, Eq. Guinea, Greece, Greenland, Guatemala, Guyana, Honduras, Croatia, Haiti, Hungary, Indonesia, India, Ireland, Iran, Iraq, Iceland, Israel, Italy, Jamaica, Jordan, Japan, Kazakhstan, Kenya, Kyrgyzstan, Cambodia, S. Korea, Kuwait, Laos, Lebanon, Liberia, Libya, Sri Lanka, Lesotho, Lithuania, Luxembourg, Latvia, Morocco, Moldova, Madagascar, Mexico, Macedonia, Mali, Myanmar, Montenegro, Mongolia, Mozambique, Mauritania, Malawi, Malaysia, Namibia, New Caledonia, Niger, Nigeria, Nicaragua, Netherlands, Norway, Nepal, New Zealand, Oman, Pakistan, Panama, Peru, Philippines, Papua New Guinea, Poland, Puerto Rico, N. Korea, Portugal, Paraguay, Qatar, Romania, Russia, Rwanda, Saudi Arabia, Sudan, S. Sudan, Senegal, Solomon Is., Sierra Leone, El Salvador, Somalia, Serbia, Suriname, Slovakia, Slovenia, Sweden, Swaziland, Syria, Chad, Togo, Thailand, Tajikistan, Turkmenistan, East Timor, Trinidad and Tobago, Tunisia, Turkey, Taiwan, Tanzania, Uganda, Ukraine, Uruguay, United States, Uzbekistan, Venezuela, Vietnam, Vanuatu, West Bank, Yemen, South Africa, Zambia, Zimbabwe | . | Custom bounding box . | Type: STRING | Default value: | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#physical_location",
    "relUrl": "/user/fields.html#physical_location"
  },"35": {
    "doc": "Fields",
    "title": "PRIMITIVE",
    "content": "List of other fields . Description: List of objects . Class: ca.breakpoints.datamaker.model.field.type.ArrayField . Configuration: . | Number of elements . | Type: NUMERIC | Default value: 5 | Possible values: | . | Field element . | Type: FIELD | Default value: None | Possible values: | . | . Big Integer Field . Description: Big integer . Class: ca.breakpoints.datamaker.model.field.type.BigIntegerField . Configuration: . | Bit length . | Type: NUMERIC | Default value: 64 | Possible values: | . | Minimum value . | Type: STRING | Default value: 0 | Possible values: | . | Maximum value . | Type: STRING | Default value: 9999999999999999999999999999999999999999999999 | Possible values: | . | Negative values . | Type: BOOLEAN | Default value: False | Possible values: | . | . Boolean value . Description: Boolean value . Class: ca.breakpoints.datamaker.model.field.type.BooleanField . Configuration: . Bytes Field . Description: Bytes . Class: ca.breakpoints.datamaker.model.field.type.BytesField . Configuration: . | Bytes length . | Type: NUMERIC | Default value: 25 | Possible values: | . | . Date Time . Description: Date . Class: ca.breakpoints.datamaker.model.field.type.DateTimeField . Configuration: . | Timezone . | Type: STRING | Default value: UTC | Possible values: Asia/Aden, America/Cuiaba, Etc/GMT+9, Etc/GMT+8, Africa/Nairobi, America/Marigot, Asia/Aqtau, Pacific/Kwajalein, America/El_Salvador, Asia/Pontianak, Africa/Cairo, Pacific/Pago_Pago, Africa/Mbabane, Asia/Kuching, Pacific/Honolulu, Pacific/Rarotonga, America/Guatemala, Australia/Hobart, Europe/London, America/Belize, America/Panama, Asia/Chungking, America/Managua, America/Indiana/Petersburg, Asia/Yerevan, Europe/Brussels, GMT, Europe/Warsaw, America/Chicago, Asia/Kashgar, Chile/Continental, Pacific/Yap, CET, Etc/GMT-1, Etc/GMT-0, Europe/Jersey, America/Tegucigalpa, Etc/GMT-5, Europe/Istanbul, America/Eirunepe, Etc/GMT-4, America/Miquelon, Etc/GMT-3, Europe/Luxembourg, Etc/GMT-2, Etc/GMT-9, America/Argentina/Catamarca, Etc/GMT-8, Etc/GMT-7, Etc/GMT-6, Europe/Zaporozhye, Canada/Yukon, Canada/Atlantic, Atlantic/St_Helena, Australia/Tasmania, Libya, Europe/Guernsey, America/Grand_Turk, Asia/Samarkand, America/Argentina/Cordoba, Asia/Phnom_Penh, Africa/Kigali, Asia/Almaty, US/Alaska, Asia/Dubai, Europe/Isle_of_Man, America/Araguaina, Cuba, Asia/Novosibirsk, America/Argentina/Salta, Etc/GMT+3, Africa/Tunis, Etc/GMT+2, Etc/GMT+1, Pacific/Fakaofo, Africa/Tripoli, Etc/GMT+0, Israel, Africa/Banjul, Etc/GMT+7, Indian/Comoro, Etc/GMT+6, Etc/GMT+5, Etc/GMT+4, Pacific/Port_Moresby, US/Arizona, Antarctica/Syowa, Indian/Reunion, Pacific/Palau, Europe/Kaliningrad, America/Montevideo, Africa/Windhoek, Asia/Karachi, Africa/Mogadishu, Australia/Perth, Brazil/East, Etc/GMT, Asia/Chita, Pacific/Easter, Antarctica/Davis, Antarctica/McMurdo, Asia/Macao, America/Manaus, Africa/Freetown, Europe/Bucharest, Asia/Tomsk, America/Argentina/Mendoza, Asia/Macau, Europe/Malta, Mexico/BajaSur, Pacific/Tahiti, Africa/Asmera, Europe/Busingen, America/Argentina/Rio_Gallegos, Africa/Malabo, Europe/Skopje, America/Catamarca, America/Godthab, Europe/Sarajevo, Australia/ACT, GB-Eire, Africa/Lagos, America/Cordoba, Europe/Rome, Asia/Dacca, Indian/Mauritius, Pacific/Samoa, America/Regina, America/Fort_Wayne, America/Dawson_Creek, Africa/Algiers, Europe/Mariehamn, America/St_Johns, America/St_Thomas, Europe/Zurich, America/Anguilla, Asia/Dili, America/Denver, Africa/Bamako, Europe/Saratov, GB, Mexico/General, Pacific/Wallis, Europe/Gibraltar, Africa/Conakry, Africa/Lubumbashi, Asia/Istanbul, America/Havana, NZ-CHAT, Asia/Choibalsan, America/Porto_Acre, Asia/Omsk, Europe/Vaduz, US/Michigan, Asia/Dhaka, America/Barbados, Europe/Tiraspol, Atlantic/Cape_Verde, Asia/Yekaterinburg, America/Louisville, Pacific/Johnston, Pacific/Chatham, Europe/Ljubljana, America/Sao_Paulo, Asia/Jayapura, America/Curacao, Asia/Dushanbe, America/Guyana, America/Guayaquil, America/Martinique, Portugal, Europe/Berlin, Europe/Moscow, Europe/Chisinau, America/Puerto_Rico, America/Rankin_Inlet, Pacific/Ponape, Europe/Stockholm, Europe/Budapest, America/Argentina/Jujuy, Australia/Eucla, Asia/Shanghai, Universal, Europe/Zagreb, America/Port_of_Spain, Europe/Helsinki, Asia/Beirut, Asia/Tel_Aviv, Pacific/Bougainville, US/Central, Africa/Sao_Tome, Indian/Chagos, America/Cayenne, Asia/Yakutsk, Pacific/Galapagos, Australia/North, Europe/Paris, Africa/Ndjamena, Pacific/Fiji, America/Rainy_River, Indian/Maldives, Australia/Yancowinna, SystemV/AST4, Asia/Oral, America/Yellowknife, Pacific/Enderbury, America/Juneau, Australia/Victoria, America/Indiana/Vevay, Asia/Tashkent, Asia/Jakarta, Africa/Ceuta, Asia/Barnaul, America/Recife, America/Buenos_Aires, America/Noronha, America/Swift_Current, Australia/Adelaide, America/Metlakatla, Africa/Djibouti, America/Paramaribo, Asia/Qostanay, Europe/Simferopol, Europe/Sofia, Africa/Nouakchott, Europe/Prague, America/Indiana/Vincennes, Antarctica/Mawson, America/Kralendijk, Antarctica/Troll, Europe/Samara, Indian/Christmas, America/Antigua, Pacific/Gambier, America/Indianapolis, America/Inuvik, America/Iqaluit, Pacific/Funafuti, UTC, Antarctica/Macquarie, Canada/Pacific, America/Moncton, Africa/Gaborone, Pacific/Chuuk, Asia/Pyongyang, America/St_Vincent, Asia/Gaza, Etc/Universal, PST8PDT, Atlantic/Faeroe, Asia/Qyzylorda, Canada/Newfoundland, America/Kentucky/Louisville, America/Yakutat, Asia/Ho_Chi_Minh, Antarctica/Casey, Europe/Copenhagen, Africa/Asmara, Atlantic/Azores, Europe/Vienna, ROK, Pacific/Pitcairn, America/Mazatlan, Australia/Queensland, Pacific/Nauru, Europe/Tirane, Asia/Kolkata, SystemV/MST7, Australia/Canberra, MET, Australia/Broken_Hill, Europe/Riga, America/Dominica, Africa/Abidjan, America/Mendoza, America/Santarem, Kwajalein, America/Asuncion, Asia/Ulan_Bator, NZ, America/Boise, Australia/Currie, EST5EDT, Pacific/Guam, Pacific/Wake, Atlantic/Bermuda, America/Costa_Rica, America/Dawson, Asia/Chongqing, Eire, Europe/Amsterdam, America/Indiana/Knox, America/North_Dakota/Beulah, Africa/Accra, Atlantic/Faroe, Mexico/BajaNorte, America/Maceio, Etc/UCT, Pacific/Apia, GMT0, America/Atka, Pacific/Niue, Australia/Lord_Howe, Europe/Dublin, Pacific/Truk, MST7MDT, America/Monterrey, America/Nassau, America/Jamaica, Asia/Bishkek, America/Atikokan, Atlantic/Stanley, Australia/NSW, US/Hawaii, SystemV/CST6, Indian/Mahe, Asia/Aqtobe, America/Sitka, Asia/Vladivostok, Africa/Libreville, Africa/Maputo, Zulu, America/Kentucky/Monticello, Africa/El_Aaiun, Africa/Ouagadougou, America/Coral_Harbour, Pacific/Marquesas, Brazil/West, America/Aruba, America/North_Dakota/Center, America/Cayman, Asia/Ulaanbaatar, Asia/Baghdad, Europe/San_Marino, America/Indiana/Tell_City, America/Tijuana, Pacific/Saipan, SystemV/YST9, Africa/Douala, America/Chihuahua, America/Ojinaga, Asia/Hovd, America/Anchorage, Chile/EasterIsland, America/Halifax, Antarctica/Rothera, America/Indiana/Indianapolis, US/Mountain, Asia/Damascus, America/Argentina/San_Luis, America/Santiago, Asia/Baku, America/Argentina/Ushuaia, Atlantic/Reykjavik, Africa/Brazzaville, Africa/Porto-Novo, America/La_Paz, Antarctica/DumontDUrville, Asia/Taipei, Antarctica/South_Pole, Asia/Manila, Asia/Bangkok, Africa/Dar_es_Salaam, Poland, Atlantic/Madeira, Antarctica/Palmer, America/Thunder_Bay, Africa/Addis_Ababa, Asia/Yangon, Europe/Uzhgorod, Brazil/DeNoronha, Asia/Ashkhabad, Etc/Zulu, America/Indiana/Marengo, America/Creston, America/Punta_Arenas, America/Mexico_City, Antarctica/Vostok, Asia/Jerusalem, Europe/Andorra, US/Samoa, PRC, Asia/Vientiane, Pacific/Kiritimati, America/Matamoros, America/Blanc-Sablon, Asia/Riyadh, Iceland, Pacific/Pohnpei, Asia/Ujung_Pandang, Atlantic/South_Georgia, Europe/Lisbon, Asia/Harbin, Europe/Oslo, Asia/Novokuznetsk, CST6CDT, Atlantic/Canary, America/Knox_IN, Asia/Kuwait, SystemV/HST10, Pacific/Efate, Africa/Lome, America/Bogota, America/Menominee, America/Adak, Pacific/Norfolk, Europe/Kirov, America/Resolute, Pacific/Tarawa, Africa/Kampala, Asia/Krasnoyarsk, Greenwich, SystemV/EST5, America/Edmonton, Europe/Podgorica, Australia/South, Canada/Central, Africa/Bujumbura, America/Santo_Domingo, US/Eastern, Europe/Minsk, Pacific/Auckland, Africa/Casablanca, America/Glace_Bay, Canada/Eastern, Asia/Qatar, Europe/Kiev, Singapore, Asia/Magadan, SystemV/PST8, America/Port-au-Prince, Europe/Belfast, America/St_Barthelemy, Asia/Ashgabat, Africa/Luanda, America/Nipigon, Atlantic/Jan_Mayen, Brazil/Acre, Asia/Muscat, Asia/Bahrain, Europe/Vilnius, America/Fortaleza, Etc/GMT0, US/East-Indiana, America/Hermosillo, America/Cancun, Africa/Maseru, Pacific/Kosrae, Africa/Kinshasa, Asia/Kathmandu, Asia/Seoul, Australia/Sydney, America/Lima, Australia/LHI, America/St_Lucia, Europe/Madrid, America/Bahia_Banderas, America/Montserrat, Asia/Brunei, America/Santa_Isabel, Canada/Mountain, America/Cambridge_Bay, Asia/Colombo, Australia/West, Indian/Antananarivo, Australia/Brisbane, Indian/Mayotte, US/Indiana-Starke, Asia/Urumqi, US/Aleutian, Europe/Volgograd, America/Lower_Princes, America/Vancouver, Africa/Blantyre, America/Rio_Branco, America/Danmarkshavn, America/Detroit, America/Thule, Africa/Lusaka, Asia/Hong_Kong, Iran, America/Argentina/La_Rioja, Africa/Dakar, SystemV/CST6CDT, America/Tortola, America/Porto_Velho, Asia/Sakhalin, Etc/GMT+10, America/Scoresbysund, Asia/Kamchatka, Asia/Thimbu, Africa/Harare, Etc/GMT+12, Etc/GMT+11, Navajo, America/Nome, Europe/Tallinn, Turkey, Africa/Khartoum, Africa/Johannesburg, Africa/Bangui, Europe/Belgrade, Jamaica, Africa/Bissau, Asia/Tehran, WET, Europe/Astrakhan, Africa/Juba, America/Campo_Grande, America/Belem, Etc/Greenwich, Asia/Saigon, America/Ensenada, Pacific/Midway, America/Jujuy, Africa/Timbuktu, America/Bahia, America/Goose_Bay, America/Virgin, America/Pangnirtung, Asia/Katmandu, America/Phoenix, Africa/Niamey, America/Whitehorse, Pacific/Noumea, Asia/Tbilisi, America/Montreal, Asia/Makassar, America/Argentina/San_Juan, Hongkong, UCT, Asia/Nicosia, America/Indiana/Winamac, SystemV/MST7MDT, America/Argentina/ComodRivadavia, America/Boa_Vista, America/Grenada, Asia/Atyrau, Australia/Darwin, Asia/Khandyga, Asia/Kuala_Lumpur, Asia/Famagusta, Asia/Thimphu, Asia/Rangoon, Europe/Bratislava, Asia/Calcutta, America/Argentina/Tucuman, Asia/Kabul, Indian/Cocos, Japan, Pacific/Tongatapu, America/New_York, Etc/GMT-12, Etc/GMT-11, America/Nuuk, Etc/GMT-10, SystemV/YST9YDT, Europe/Ulyanovsk, Etc/GMT-14, Etc/GMT-13, W-SU, America/Merida, EET, America/Rosario, Canada/Saskatchewan, America/St_Kitts, Arctic/Longyearbyen, America/Fort_Nelson, America/Caracas, America/Guadeloupe, Asia/Hebron, Indian/Kerguelen, SystemV/PST8PDT, Africa/Monrovia, Asia/Ust-Nera, Egypt, Asia/Srednekolymsk, America/North_Dakota/New_Salem, Asia/Anadyr, Australia/Melbourne, Asia/Irkutsk, America/Shiprock, America/Winnipeg, Europe/Vatican, Asia/Amman, Etc/UTC, SystemV/AST4ADT, Asia/Tokyo, America/Toronto, Asia/Singapore, Australia/Lindeman, America/Los_Angeles, SystemV/EST5EDT, Pacific/Majuro, America/Argentina/Buenos_Aires, Europe/Nicosia, Pacific/Guadalcanal, Europe/Athens, US/Pacific, Europe/Monaco | . | Datetime type . | Type: STRING | Default value: CURRENT | Possible values: DATE_OF_BIRTH, PAST, FUTURE, CURRENT, DATE_ONLY, TIME_ONLY | . | Start date . | Type: DATE | Default value: | Possible values: | . | Start time . | Type: TIME | Default value: 00:00:00 | Possible values: | . | End date . | Type: DATE | Default value: | Possible values: | . | End time . | Type: TIME | Default value: 23:59:59 | Possible values: | . | Time unit . | Type: STRING | Default value: DAYS | Possible values: NANOSECONDS, MICROSECONDS, MILLISECONDS, SECONDS, MINUTES, HOURS, DAYS | . | At most (used with past and future option) . | Type: NUMERIC | Default value: 365 | Possible values: | . | . Big decimal . Description: Big decimal number. Useful for money calculation. Class: ca.breakpoints.datamaker.model.field.type.DecimalField . Configuration: . | Minimum value . | Type: NUMERIC | Default value: 0.0 | Possible values: | . | Maximum value . | Type: NUMERIC | Default value: 1.7976931348623157e+308 | Possible values: | . | . Floating point: double precision . Description: Floating point: double precision . Class: ca.breakpoints.datamaker.model.field.type.DoubleField . Configuration: . | Minimum value . | Type: NUMERIC | Default value: 0.0 | Possible values: -9223372036854775808, 0 | . | Maximum value . | Type: NUMERIC | Default value: 1000.0 | Possible values: 0, 9223372036854775807 | . | Maximum number of decimals . | Type: NUMERIC | Default value: 16 | Possible values: | . | . Duration Field . Description: Duration . Class: ca.breakpoints.datamaker.model.field.type.DurationField . Configuration: . | Format . | Type: STRING | Default value: P5DT5H5M5.4S | Possible values: | . | . Floating point: simple precision . Description: Floating point . Class: ca.breakpoints.datamaker.model.field.type.FloatField . Configuration: . | Minimum value . | Type: NUMERIC | Default value: 0 | Possible values: 0, 1.4e-45 | . | Maximum value . | Type: NUMERIC | Default value: 32767 | Possible values: 0, 3.4028235e+38 | . | Maximum number of decimals . | Type: NUMERIC | Default value: 7 | Possible values: 0, 7 | . | . Integer Field . Description: Number . Class: ca.breakpoints.datamaker.model.field.type.IntegerField . Configuration: . | Minimum value . | Type: NUMERIC | Default value: 0 | Possible values: -2147483648, 0 | . | Maximum value . | Type: NUMERIC | Default value: 100000 | Possible values: 0, 2147483647 | . | Negative value . | Type: BOOLEAN | Default value: False | Possible values: | . | . Long . Description: Number . Class: ca.breakpoints.datamaker.model.field.type.LongField . Configuration: . | Minimum value . | Type: NUMERIC | Default value: 0 | Possible values: -2147483648, 0 | . | Maximum value . | Type: NUMERIC | Default value: 1000 | Possible values: 0, 2147483647 | . | Negative value . | Type: BOOLEAN | Default value: False | Possible values: | . | . Null value . Description: Null value field . Class: ca.breakpoints.datamaker.model.field.type.NullField . Configuration: . Period Field . Description: Period . Class: ca.breakpoints.datamaker.model.field.type.PeriodField . Configuration: . | Format . | Type: STRING | Default value: PnYnMnD | Possible values: | . | . Stock Field . Description: Market stock . Class: ca.breakpoints.datamaker.model.field.type.StockField . Configuration: . String Field . Description: Random chain of characters . Class: ca.breakpoints.datamaker.model.field.type.StringField . Configuration: . | Length . | Type: NUMERIC | Default value: 25 | Possible values: | . | Alpha numeric . | Type: BOOLEAN | Default value: True | Possible values: | . | Ascii only . | Type: BOOLEAN | Default value: True | Possible values: | . | . Timestamp Field . Description: Number of seconds after epoch . Class: ca.breakpoints.datamaker.model.field.type.TimestampField . Configuration: . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/fields.html#primitive",
    "relUrl": "/user/fields.html#primitive"
  },"36": {
    "doc": "File Manager",
    "title": "Resource File Manager",
    "content": "The file manager can be used to manage all custom or external files required by the application to run jobs. This includes databases drivers, sink libraries, security or encryption keys. ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/file_manager.html#resource-file-manager",
    "relUrl": "/admin/file_manager.html#resource-file-manager"
  },"37": {
    "doc": "File Manager",
    "title": "Landing page",
    "content": "File Manager . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/file_manager.html#landing-page",
    "relUrl": "/admin/file_manager.html#landing-page"
  },"38": {
    "doc": "File Manager",
    "title": "Edit page",
    "content": "Create resource file . File Type: . | JAR: use this type for uploading libraries (ex: JDBC, custom file format, sinks). These files will be added to the classpath. | You need to restart the service in order to use the uploaded artefact. | . | KEYTAB: Kerberos principals and encrypted keys (ex: Hadoop, SOLR) | JAAS: Java Authentication and Authorization Service (ex: Kafka) | JKS: Java KeyStore | RESOURCE: properties | OTHER: any kind of file | . You can copy paste the content of the file or upload it. ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/file_manager.html#edit-page",
    "relUrl": "/admin/file_manager.html#edit-page"
  },"39": {
    "doc": "File Manager",
    "title": "File Manager",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/file_manager.html",
    "relUrl": "/admin/file_manager.html"
  },"40": {
    "doc": "Formatters",
    "title": "Formatters",
    "content": "Camel Case . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.CamelCaseFormatter . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | To upper case . | Type: BOOLEAN | Default value: False | Possible values: | . | . Capitalize . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.CapitalizeFormatter . Configuration: . | Capitalize fully . | Type: BOOLEAN | Default value: False | Possible values: | . | . Credit Card . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.CreditCardFormatter . Configuration: . | Credit card format . | Type: STRING | Default value: NONE | Possible values: SPACES, HYPHENS, NONE | . | . Date Time Numeric . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.DateTimeNumericFormatter . Configuration: . | Timezone . | Type: STRING | Default value: Long | Possible values: Integer, Long | . | . Date Time String . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.DateTimeStringFormatter . Configuration: . | Timezone . | Type: STRING | Default value: UTC | Possible values: Asia/Aden, America/Cuiaba, Etc/GMT+9, Etc/GMT+8, Africa/Nairobi, America/Marigot, Asia/Aqtau, Pacific/Kwajalein, America/El_Salvador, Asia/Pontianak, Africa/Cairo, Pacific/Pago_Pago, Africa/Mbabane, Asia/Kuching, Pacific/Honolulu, Pacific/Rarotonga, America/Guatemala, Australia/Hobart, Europe/London, America/Belize, America/Panama, Asia/Chungking, America/Managua, America/Indiana/Petersburg, Asia/Yerevan, Europe/Brussels, GMT, Europe/Warsaw, America/Chicago, Asia/Kashgar, Chile/Continental, Pacific/Yap, CET, Etc/GMT-1, Etc/GMT-0, Europe/Jersey, America/Tegucigalpa, Etc/GMT-5, Europe/Istanbul, America/Eirunepe, Etc/GMT-4, America/Miquelon, Etc/GMT-3, Europe/Luxembourg, Etc/GMT-2, Etc/GMT-9, America/Argentina/Catamarca, Etc/GMT-8, Etc/GMT-7, Etc/GMT-6, Europe/Zaporozhye, Canada/Yukon, Canada/Atlantic, Atlantic/St_Helena, Australia/Tasmania, Libya, Europe/Guernsey, America/Grand_Turk, Asia/Samarkand, America/Argentina/Cordoba, Asia/Phnom_Penh, Africa/Kigali, Asia/Almaty, US/Alaska, Asia/Dubai, Europe/Isle_of_Man, America/Araguaina, Cuba, Asia/Novosibirsk, America/Argentina/Salta, Etc/GMT+3, Africa/Tunis, Etc/GMT+2, Etc/GMT+1, Pacific/Fakaofo, Africa/Tripoli, Etc/GMT+0, Israel, Africa/Banjul, Etc/GMT+7, Indian/Comoro, Etc/GMT+6, Etc/GMT+5, Etc/GMT+4, Pacific/Port_Moresby, US/Arizona, Antarctica/Syowa, Indian/Reunion, Pacific/Palau, Europe/Kaliningrad, America/Montevideo, Africa/Windhoek, Asia/Karachi, Africa/Mogadishu, Australia/Perth, Brazil/East, Etc/GMT, Asia/Chita, Pacific/Easter, Antarctica/Davis, Antarctica/McMurdo, Asia/Macao, America/Manaus, Africa/Freetown, Europe/Bucharest, Asia/Tomsk, America/Argentina/Mendoza, Asia/Macau, Europe/Malta, Mexico/BajaSur, Pacific/Tahiti, Africa/Asmera, Europe/Busingen, America/Argentina/Rio_Gallegos, Africa/Malabo, Europe/Skopje, America/Catamarca, America/Godthab, Europe/Sarajevo, Australia/ACT, GB-Eire, Africa/Lagos, America/Cordoba, Europe/Rome, Asia/Dacca, Indian/Mauritius, Pacific/Samoa, America/Regina, America/Fort_Wayne, America/Dawson_Creek, Africa/Algiers, Europe/Mariehamn, America/St_Johns, America/St_Thomas, Europe/Zurich, America/Anguilla, Asia/Dili, America/Denver, Africa/Bamako, Europe/Saratov, GB, Mexico/General, Pacific/Wallis, Europe/Gibraltar, Africa/Conakry, Africa/Lubumbashi, Asia/Istanbul, America/Havana, NZ-CHAT, Asia/Choibalsan, America/Porto_Acre, Asia/Omsk, Europe/Vaduz, US/Michigan, Asia/Dhaka, America/Barbados, Europe/Tiraspol, Atlantic/Cape_Verde, Asia/Yekaterinburg, America/Louisville, Pacific/Johnston, Pacific/Chatham, Europe/Ljubljana, America/Sao_Paulo, Asia/Jayapura, America/Curacao, Asia/Dushanbe, America/Guyana, America/Guayaquil, America/Martinique, Portugal, Europe/Berlin, Europe/Moscow, Europe/Chisinau, America/Puerto_Rico, America/Rankin_Inlet, Pacific/Ponape, Europe/Stockholm, Europe/Budapest, America/Argentina/Jujuy, Australia/Eucla, Asia/Shanghai, Universal, Europe/Zagreb, America/Port_of_Spain, Europe/Helsinki, Asia/Beirut, Asia/Tel_Aviv, Pacific/Bougainville, US/Central, Africa/Sao_Tome, Indian/Chagos, America/Cayenne, Asia/Yakutsk, Pacific/Galapagos, Australia/North, Europe/Paris, Africa/Ndjamena, Pacific/Fiji, America/Rainy_River, Indian/Maldives, Australia/Yancowinna, SystemV/AST4, Asia/Oral, America/Yellowknife, Pacific/Enderbury, America/Juneau, Australia/Victoria, America/Indiana/Vevay, Asia/Tashkent, Asia/Jakarta, Africa/Ceuta, Asia/Barnaul, America/Recife, America/Buenos_Aires, America/Noronha, America/Swift_Current, Australia/Adelaide, America/Metlakatla, Africa/Djibouti, America/Paramaribo, Asia/Qostanay, Europe/Simferopol, Europe/Sofia, Africa/Nouakchott, Europe/Prague, America/Indiana/Vincennes, Antarctica/Mawson, America/Kralendijk, Antarctica/Troll, Europe/Samara, Indian/Christmas, America/Antigua, Pacific/Gambier, America/Indianapolis, America/Inuvik, America/Iqaluit, Pacific/Funafuti, UTC, Antarctica/Macquarie, Canada/Pacific, America/Moncton, Africa/Gaborone, Pacific/Chuuk, Asia/Pyongyang, America/St_Vincent, Asia/Gaza, Etc/Universal, PST8PDT, Atlantic/Faeroe, Asia/Qyzylorda, Canada/Newfoundland, America/Kentucky/Louisville, America/Yakutat, Asia/Ho_Chi_Minh, Antarctica/Casey, Europe/Copenhagen, Africa/Asmara, Atlantic/Azores, Europe/Vienna, ROK, Pacific/Pitcairn, America/Mazatlan, Australia/Queensland, Pacific/Nauru, Europe/Tirane, Asia/Kolkata, SystemV/MST7, Australia/Canberra, MET, Australia/Broken_Hill, Europe/Riga, America/Dominica, Africa/Abidjan, America/Mendoza, America/Santarem, Kwajalein, America/Asuncion, Asia/Ulan_Bator, NZ, America/Boise, Australia/Currie, EST5EDT, Pacific/Guam, Pacific/Wake, Atlantic/Bermuda, America/Costa_Rica, America/Dawson, Asia/Chongqing, Eire, Europe/Amsterdam, America/Indiana/Knox, America/North_Dakota/Beulah, Africa/Accra, Atlantic/Faroe, Mexico/BajaNorte, America/Maceio, Etc/UCT, Pacific/Apia, GMT0, America/Atka, Pacific/Niue, Australia/Lord_Howe, Europe/Dublin, Pacific/Truk, MST7MDT, America/Monterrey, America/Nassau, America/Jamaica, Asia/Bishkek, America/Atikokan, Atlantic/Stanley, Australia/NSW, US/Hawaii, SystemV/CST6, Indian/Mahe, Asia/Aqtobe, America/Sitka, Asia/Vladivostok, Africa/Libreville, Africa/Maputo, Zulu, America/Kentucky/Monticello, Africa/El_Aaiun, Africa/Ouagadougou, America/Coral_Harbour, Pacific/Marquesas, Brazil/West, America/Aruba, America/North_Dakota/Center, America/Cayman, Asia/Ulaanbaatar, Asia/Baghdad, Europe/San_Marino, America/Indiana/Tell_City, America/Tijuana, Pacific/Saipan, SystemV/YST9, Africa/Douala, America/Chihuahua, America/Ojinaga, Asia/Hovd, America/Anchorage, Chile/EasterIsland, America/Halifax, Antarctica/Rothera, America/Indiana/Indianapolis, US/Mountain, Asia/Damascus, America/Argentina/San_Luis, America/Santiago, Asia/Baku, America/Argentina/Ushuaia, Atlantic/Reykjavik, Africa/Brazzaville, Africa/Porto-Novo, America/La_Paz, Antarctica/DumontDUrville, Asia/Taipei, Antarctica/South_Pole, Asia/Manila, Asia/Bangkok, Africa/Dar_es_Salaam, Poland, Atlantic/Madeira, Antarctica/Palmer, America/Thunder_Bay, Africa/Addis_Ababa, Asia/Yangon, Europe/Uzhgorod, Brazil/DeNoronha, Asia/Ashkhabad, Etc/Zulu, America/Indiana/Marengo, America/Creston, America/Punta_Arenas, America/Mexico_City, Antarctica/Vostok, Asia/Jerusalem, Europe/Andorra, US/Samoa, PRC, Asia/Vientiane, Pacific/Kiritimati, America/Matamoros, America/Blanc-Sablon, Asia/Riyadh, Iceland, Pacific/Pohnpei, Asia/Ujung_Pandang, Atlantic/South_Georgia, Europe/Lisbon, Asia/Harbin, Europe/Oslo, Asia/Novokuznetsk, CST6CDT, Atlantic/Canary, America/Knox_IN, Asia/Kuwait, SystemV/HST10, Pacific/Efate, Africa/Lome, America/Bogota, America/Menominee, America/Adak, Pacific/Norfolk, Europe/Kirov, America/Resolute, Pacific/Tarawa, Africa/Kampala, Asia/Krasnoyarsk, Greenwich, SystemV/EST5, America/Edmonton, Europe/Podgorica, Australia/South, Canada/Central, Africa/Bujumbura, America/Santo_Domingo, US/Eastern, Europe/Minsk, Pacific/Auckland, Africa/Casablanca, America/Glace_Bay, Canada/Eastern, Asia/Qatar, Europe/Kiev, Singapore, Asia/Magadan, SystemV/PST8, America/Port-au-Prince, Europe/Belfast, America/St_Barthelemy, Asia/Ashgabat, Africa/Luanda, America/Nipigon, Atlantic/Jan_Mayen, Brazil/Acre, Asia/Muscat, Asia/Bahrain, Europe/Vilnius, America/Fortaleza, Etc/GMT0, US/East-Indiana, America/Hermosillo, America/Cancun, Africa/Maseru, Pacific/Kosrae, Africa/Kinshasa, Asia/Kathmandu, Asia/Seoul, Australia/Sydney, America/Lima, Australia/LHI, America/St_Lucia, Europe/Madrid, America/Bahia_Banderas, America/Montserrat, Asia/Brunei, America/Santa_Isabel, Canada/Mountain, America/Cambridge_Bay, Asia/Colombo, Australia/West, Indian/Antananarivo, Australia/Brisbane, Indian/Mayotte, US/Indiana-Starke, Asia/Urumqi, US/Aleutian, Europe/Volgograd, America/Lower_Princes, America/Vancouver, Africa/Blantyre, America/Rio_Branco, America/Danmarkshavn, America/Detroit, America/Thule, Africa/Lusaka, Asia/Hong_Kong, Iran, America/Argentina/La_Rioja, Africa/Dakar, SystemV/CST6CDT, America/Tortola, America/Porto_Velho, Asia/Sakhalin, Etc/GMT+10, America/Scoresbysund, Asia/Kamchatka, Asia/Thimbu, Africa/Harare, Etc/GMT+12, Etc/GMT+11, Navajo, America/Nome, Europe/Tallinn, Turkey, Africa/Khartoum, Africa/Johannesburg, Africa/Bangui, Europe/Belgrade, Jamaica, Africa/Bissau, Asia/Tehran, WET, Europe/Astrakhan, Africa/Juba, America/Campo_Grande, America/Belem, Etc/Greenwich, Asia/Saigon, America/Ensenada, Pacific/Midway, America/Jujuy, Africa/Timbuktu, America/Bahia, America/Goose_Bay, America/Virgin, America/Pangnirtung, Asia/Katmandu, America/Phoenix, Africa/Niamey, America/Whitehorse, Pacific/Noumea, Asia/Tbilisi, America/Montreal, Asia/Makassar, America/Argentina/San_Juan, Hongkong, UCT, Asia/Nicosia, America/Indiana/Winamac, SystemV/MST7MDT, America/Argentina/ComodRivadavia, America/Boa_Vista, America/Grenada, Asia/Atyrau, Australia/Darwin, Asia/Khandyga, Asia/Kuala_Lumpur, Asia/Famagusta, Asia/Thimphu, Asia/Rangoon, Europe/Bratislava, Asia/Calcutta, America/Argentina/Tucuman, Asia/Kabul, Indian/Cocos, Japan, Pacific/Tongatapu, America/New_York, Etc/GMT-12, Etc/GMT-11, America/Nuuk, Etc/GMT-10, SystemV/YST9YDT, Europe/Ulyanovsk, Etc/GMT-14, Etc/GMT-13, W-SU, America/Merida, EET, America/Rosario, Canada/Saskatchewan, America/St_Kitts, Arctic/Longyearbyen, America/Fort_Nelson, America/Caracas, America/Guadeloupe, Asia/Hebron, Indian/Kerguelen, SystemV/PST8PDT, Africa/Monrovia, Asia/Ust-Nera, Egypt, Asia/Srednekolymsk, America/North_Dakota/New_Salem, Asia/Anadyr, Australia/Melbourne, Asia/Irkutsk, America/Shiprock, America/Winnipeg, Europe/Vatican, Asia/Amman, Etc/UTC, SystemV/AST4ADT, Asia/Tokyo, America/Toronto, Asia/Singapore, Australia/Lindeman, America/Los_Angeles, SystemV/EST5EDT, Pacific/Majuro, America/Argentina/Buenos_Aires, Europe/Nicosia, Pacific/Guadalcanal, Europe/Athens, US/Pacific, Europe/Monaco | . | Output format . | Type: STRING | Default value: yyyy-MM-dd’T’HH:mm:ss.SSSX | Possible values: | . | . Decimal . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.DecimalFormatter . Configuration: . | Number pattern . | Type: STRING | Default value: #,###.00 | Possible values: | . | Rounding mode . | Type: STRING | Default value: HALF_EVEN | Possible values: UP, DOWN, CEILING, FLOOR, HALF_UP, HALF_DOWN, HALF_EVEN, UNNECESSARY | . | . Geography . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.GeographyFormatter . Configuration: . | Format type . | Type: STRING | Default value: NONE | Possible values: GEO_JSON, LAT_LONG, LATITUDE, LONGITUDE, AS_STRING, AS_DOUBLE, NONE | . | Template . | Type: STRING | Default value: { “type”: “%s”, “coordinates”: %s } | Possible values: | . | . Identification Number . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.IdentificationNumberFormatter . Configuration: . Json . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.JsonFormatter . Configuration: . Lower Case . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.LowerCaseFormatter . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | . Number . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.NumberFormatter . Configuration: . | Number pattern . | Type: STRING | Default value: #,###.00 | Possible values: | . | Rounding mode . | Type: STRING | Default value: HALF_EVEN | Possible values: UP, DOWN, CEILING, FLOOR, HALF_UP, HALF_DOWN, HALF_EVEN, UNNECESSARY | . | . Phone . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.PhoneFormatter . Configuration: . Snake Case . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.SnakeCaseFormatter . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | To upper case . | Type: BOOLEAN | Default value: False | Possible values: | . | . String . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.StringFormatter . Configuration: . | String format . | Type: STRING | Default value: %s | Possible values: | . | . Upper Case . Description: . Class: ca.breakpoints.datamaker.model.field.formatter.UpperCaseFormatter . Configuration: . | Locale . | Type: STRING | Default value: en | Possible values: en, fr | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/formatters.html",
    "relUrl": "/user/formatters.html"
  },"41": {
    "doc": "Generators",
    "title": "Generators",
    "content": "Avro . Description: . Class: ca.breakpoints.datamaker.generator.AvroGenerator . Configuration: . | Compress content . | Type: BOOLEAN | Default value: False | Possible values: True, False | . | Codec . | Type: STRING | Default value: null | Possible values: null, deflate, bzip2, xz, zstandard, snappy | . | . Bytes . Description: . Class: ca.breakpoints.datamaker.generator.BytesGenerator . Configuration: . Csv . Description: . Class: ca.breakpoints.datamaker.generator.CsvGenerator . Configuration: . | File encoding . | Type: STRING | Default value: UTF-8 | Possible values: | . | Delimiter . | Type: STRING | Default value: , | Possible values: | . | Quote char . | Type: STRING | Default value: “ | Possible values: | . | Escape char . | Type: STRING | Default value: “ | Possible values: | . | Line ending . | Type: STRING | Default value: . | Possible values: | . | Null value . | Type: STRING | Default value: | Possible values: | . | Quote all . | Type: BOOLEAN | Default value: False | Possible values: True, False | . | . Excel . Description: . Class: ca.breakpoints.datamaker.generator.ExcelGenerator . Configuration: . | Date format . | Type: STRING | Default value: yyyy/m/d h:mm | Possible values: | . | . Json . Description: . Class: ca.breakpoints.datamaker.generator.JsonGenerator . Configuration: . | Line ending . | Type: STRING | Default value: . | Possible values: | . | . Parquet . Description: . Class: ca.breakpoints.datamaker.generator.ParquetGenerator . Configuration: . | Compress content . | Type: BOOLEAN | Default value: False | Possible values: True, False | . | Codec . | Type: STRING | Default value: UNCOMPRESSED | Possible values: UNCOMPRESSED, SNAPPY, GZIP, LZO, BROTLI, LZ4, ZSTD | . | . Passthrough . Description: . Class: ca.breakpoints.datamaker.generator.PassthroughGenerator . Configuration: . Pdf . Description: . Class: ca.breakpoints.datamaker.generator.PdfGenerator . Configuration: . | Template . | Type: STRING | Default value: | Possible values: | . | Number of pages . | Type: NUMERIC | Default value: | Possible values: | . | Font . | Type: STRING | Default value: TIMES_ROMAN | Possible values: COURIER, HELVETICA, TIMES_ROMAN, SYMBOL, ZAPFDINGBATS, UNDEFINED | . | Output data in a table . | Type: BOOLEAN | Default value: False | Possible values: | . | Document author . | Type: STRING | Default value: | Possible values: | . | Document creator . | Type: STRING | Default value: | Possible values: | . | Document title . | Type: EXPRESSION | Default value: #dataset.name | Possible values: | . | Document subject . | Type: EXPRESSION | Default value: | Possible values: | . | Keywords . | Type: STRING | Default value: | Possible values: | . | . Sql . Description: . Class: ca.breakpoints.datamaker.generator.SqlGenerator . Configuration: . | Managed table . | Type: BOOLEAN | Default value: False | Possible values: True, False | . | Line ending . | Type: STRING | Default value: . | Possible values: | . | SQL Dialect . | Type: STRING | Default value: SQL_1999 | Possible values: SQL_1999, SQL_2006, PL_pgSQL, Transact_SQL, PL_SQL, SQL_SERVER, POSTGRES, MYSQL, HIVE | . | . Template Data . Description: . Class: ca.breakpoints.datamaker.generator.TemplateDataGenerator . Configuration: . | Freemarker template . | Type: STRING | Default value: Dataset: ${dataset.name} &lt;#list fieldValueList as fieldValueList &gt; &lt;#list fieldValueList as fieldValue&gt; ${fieldValue.field.name}: ${fieldValue.value} &lt;/#list&gt; ======================= &lt;/#list&gt; | Possible values: | . | . Text . Description: . Class: ca.breakpoints.datamaker.generator.TextGenerator . Configuration: . | Element separator . | Type: STRING | Default value: | Possible values: | . | Key value separator . | Type: STRING | Default value: = | Possible values: | . | Output keys . | Type: BOOLEAN | Default value: false | Possible values: | . | . Xml . Description: . Class: ca.breakpoints.datamaker.generator.XmlGenerator . Configuration: . | Root element . | Type: STRING | Default value: | Possible values: | . | Encoding . | Type: STRING | Default value: UTF-8 | Possible values: | . | Pretty print . | Type: BOOLEAN | Default value: False | Possible values: True, False | . | Version . | Type: STRING | Default value: 1.0 | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/generators.html",
    "relUrl": "/user/generators.html"
  },"42": {
    "doc": "Getting Started",
    "title": "Getting started",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started.html#getting-started",
    "relUrl": "/user/getting_started.html#getting-started"
  },"43": {
    "doc": "Getting Started",
    "title": "Login",
    "content": ". Initial credentials . Username: admin Password: changeme . If you have configured an oauth provider or running on the cloud, you can click on the oauth provider to login. ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started.html#login",
    "relUrl": "/user/getting_started.html#login"
  },"44": {
    "doc": "Getting Started",
    "title": "Dashboard",
    "content": ". Header . | Burger menu: show/hide left sidebar | Dashboard: shows the home page | Click on the Documentation link to read the docs | Language: change the interface language | Search is activated by entering a search keyword (wildcard * is supported) | Click on the profile logo (top right corner) to log out or edit your profile | . Left Sidebar . You collapse or show the sidebar by clicking the arrow icon at the bottom left of the screen. You can hide the sidebar entirely by clicking on the burger menu (3 lines). The sidebar contains the following sections: . | Dashboard: home page with various statistics | Datasets: dataset creation view | Field mappings: global field mappings configuration | Jobs: data generation jobs | System: . | Health: show performance metrics | Info: build info | File manager: manage file resources | Logs: show application logs | . | Sinks: manage global sink configuration | Workspaces: workspace management view | . Main section . Displays the results, forms, metrics based on the section selected . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started.html#dashboard",
    "relUrl": "/user/getting_started.html#dashboard"
  },"45": {
    "doc": "Getting Started",
    "title": "First steps",
    "content": ". | Create a workspace that will contain your datasets and jobs | Then create a dataset manually using the UI or you can also upload a sample file to infer data types | Using your dataset as input, create a data generation job . | You will configure the output format and also the target systems where the data will be sent (sinks) | . | Last step is to verify everything is working fine from the jobs main page . | Select history to see the execution output | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started.html#first-steps",
    "relUrl": "/user/getting_started.html#first-steps"
  },"46": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started.html",
    "relUrl": "/user/getting_started.html"
  },"47": {
    "doc": "Getting Started AMI",
    "title": "Getting started Amazon AMI",
    "content": ". ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started_ami.html#getting-started-amazon-ami",
    "relUrl": "/user/getting_started_ami.html#getting-started-amazon-ami"
  },"48": {
    "doc": "Getting Started AMI",
    "title": "Login",
    "content": ". Initial credentials . Username: admin Password: [AMI Instance ID] . You can find out the AMI instance ID from the EC2 Console. If you have configured Amazon Cognito, you can click on the oauth provider to login. ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started_ami.html#login",
    "relUrl": "/user/getting_started_ami.html#login"
  },"49": {
    "doc": "Getting Started AMI",
    "title": "Dashboard",
    "content": ". Header . | Burger menu: show/hide left sidebar | Dashboard: shows the home page | Click on the Documentation link to read the docs | Language: change the interface language | Search is activated by entering a search keyword (wildcard * is supported) | Click on the profile logo (top right corner) to log out or edit your profile | . Left Sidebar . You collapse or show the sidebar by clicking the arrow icon at the bottom left of the screen. You can hide the sidebar entirely by clicking on the burger menu (3 lines). The sidebar contains the following sections: . | Dashboard: home page with various statistics | Datasets: dataset creation view | Field mappings: global field mappings configuration | Jobs: data generation jobs | System: . | Health: show performance metrics | Info: build info | File manager: manage file resources | Logs: show application logs | . | Sinks: manage global sink configuration | Workspaces: workspace management view | . Main section . Displays the results, forms, metrics based on the section selected . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started_ami.html#dashboard",
    "relUrl": "/user/getting_started_ami.html#dashboard"
  },"50": {
    "doc": "Getting Started AMI",
    "title": "First steps",
    "content": ". | Create a workspace that will contain your datasets and jobs | Then create a dataset manually using the UI or you can also upload a sample file to infer data types | Using your dataset as input, create a data generation job . | You will configure the output format and also the target systems where the data will be sent (sinks) | . | Last step is to verify everything is working fine from the jobs main page . | Select history to see the execution output | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started_ami.html#first-steps",
    "relUrl": "/user/getting_started_ami.html#first-steps"
  },"51": {
    "doc": "Getting Started AMI",
    "title": "Getting Started AMI",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/getting_started_ami.html",
    "relUrl": "/user/getting_started_ami.html"
  },"52": {
    "doc": "Architecture",
    "title": "Overall Architecture",
    "content": ". File Processor . Takes a sample data file as input and convert it automatically in a dataset representation. The current supported format are: AVRO, CSV, EXCEL, JSON, JSON Schema, PARQUET, XML, TEXT, SQL, XML, XSD . Dataset Controller . Web service to create dataset manually or programmatically. Fields Detection Service . Service that infer data type based on values, field name and schema definition (optional). Job Triggers . Jobs can be started manually or automatically (cron, random, burst). Data Generator . From an existing dataset, a data generation job will create realistic data that matches all the provided requirements. Sinks . The data is sent to a supported sink (data output). ",
    "url": "https://www.datamaker.ai/datamaker/docs/architecture/#overall-architecture",
    "relUrl": "/architecture/#overall-architecture"
  },"53": {
    "doc": "Architecture",
    "title": "Architecture",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/architecture/",
    "relUrl": "/architecture/"
  },"54": {
    "doc": "User",
    "title": "User Guide",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/#user-guide",
    "relUrl": "/user/#user-guide"
  },"55": {
    "doc": "User",
    "title": "User",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/",
    "relUrl": "/user/"
  },"56": {
    "doc": "Admin",
    "title": "Admin Guide",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/#admin-guide",
    "relUrl": "/admin/#admin-guide"
  },"57": {
    "doc": "Admin",
    "title": "Admin",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/",
    "relUrl": "/admin/"
  },"58": {
    "doc": "Developer",
    "title": "Developer Guide",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/dev/#developer-guide",
    "relUrl": "/dev/#developer-guide"
  },"59": {
    "doc": "Developer",
    "title": "Developer",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/dev/",
    "relUrl": "/dev/"
  },"60": {
    "doc": "Installation",
    "title": "Installation",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html",
    "relUrl": "/admin/install.html"
  },"61": {
    "doc": "Installation",
    "title": "Table of contents",
    "content": ". | Requirements | Downloads | Standalone (runnable jar) | Demo | Docker . | Docker Compose | . | AWS EC2 | AWS Elactic Beanstalk | Azure | Google App Engine | Tomcat | Kubernetes | Cloud (Custom) | Google Marketplace | AWS Marketplace | Azure Marketplace | Snowflake Data Marketplace | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#table-of-contents",
    "relUrl": "/admin/install.html#table-of-contents"
  },"62": {
    "doc": "Installation",
    "title": "Requirements",
    "content": ". | MySQL 8.0 . | You can use your own instance (Docker, Standalone, Cloud) | . | Java 11 . | https://adoptopenjdk.net/releases.html | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#requirements",
    "relUrl": "/admin/install.html#requirements"
  },"63": {
    "doc": "Installation",
    "title": "Downloads",
    "content": ". | Download the archive from here: https://www.datamaker.ai/downloads/releases . | Use your company/user credentials (contact support if you didn’t receive it yet) | . | Use the latest version for your target architecture . | default: self-contain service | azure: Application Service deployment | aws: Elastic Beanstalk deployment | gcp: Google Cloud Platform | war: Tomcat or J2EE container deployment | tar: Docker image | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#downloads",
    "relUrl": "/admin/install.html#downloads"
  },"64": {
    "doc": "Installation",
    "title": "Standalone (runnable jar)",
    "content": ". | Download default version from downloads site: datamaker-${VERSION}.zip | cp datamaker-*.zip /opt | cd /opt | unzip datamaker-*.zip | cd datamaker | Change or keep server port number: server.port=8080 . | Set MySQL database parameters in application.properties: spring.datasource.url=jdbc:mysql://localhost:3306/datamaker spring.datasource.driverClassName=com.mysql.cj.jdbc.Driver spring.datasource.username=root spring.datasource.password=changeme . | ./install.sh | Open up a browser and navigate to http://server_url:8080/datamaker | Login using admin / changeme credentials | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#standalone-runnable-jar",
    "relUrl": "/admin/install.html#standalone-runnable-jar"
  },"65": {
    "doc": "Installation",
    "title": "Demo",
    "content": ". | Download the archive from here: https://www.datamaker.ai/downloads/demo | cp datamaker-*.zip /opt | cd /opt | unzip datamaker-*.zip | cd datamaker | Change or keep server port number: . server.port=8080 . | The demo version contains an embedded database. To activate, edit service.conf: JAVA_OPTS='-Xmx512M -Dspring.active.profiles=demo -Dlogging.config=/opt/datamaker/logback-spring.xml -DLOGS_PATH=/opt/datamaker/logs' . | if you want to install as a service: ./install.sh | if you want to run it on demand: java -jar -Dspring.profiles.active=demo datamaker-${VERSION}.jar | Open up a browser and navigate to http://server_url:8080/datamaker | Login using admin / changeme credentials | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#demo",
    "relUrl": "/admin/install.html#demo"
  },"66": {
    "doc": "Installation",
    "title": "Docker",
    "content": ". | Download the docker image from here: https://www.datamaker.ai/downloads/releases | replace ${VERSION} by version downloaded | docker images | docker load &lt; datamaker-docker-${VERSION}.tar | Change the running port number and the database connection docker run -p 8080:8080 \\ -e SPRING_DATASOURCE_URL=jdbc:mysql://${HOSTNAME}:3306/datamaker \\ -e SPRING_DATASOURCE_USERNAME=root \\ -e SPRING_DATASOURCE_PASSWORD=changeme \\ --name datamaker datamaker:${VERSION} . | Open up a browser and navigate to http://server_url:8080/datamaker | Login using admin / changeme credentials | . Docker Compose . If you want to use a docker image of MySQL as well, use this setup. | mkdir datamaker | create docker-compose.yml . | replace ${VERSION} by version downloaded | . | . docker-compose.yml template . version: '3.3' services: db: image: mysql:8 volumes: - db_data:/var/lib/mysql # - ./init.sql:/docker-entrypoint-initdb.d/init.sql command: --default-authentication-plugin=mysql_native_password restart: always ports: - 3306:3306 environment: MYSQL_ROOT_PASSWORD: changeme MYSQL_DATABASE: datamaker service: image: datamaker:${VERSION} #restart: always depends_on: - db ports: - 8080:8080 adminer: image: adminer restart: always ports: - 8088:8080 volumes: db_data: {} . | docker compose up | Open up a browser and navigate to http://server_url:8080/datamaker | Login using admin / changeme credentials | . To delete . | docker compose down | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#docker",
    "relUrl": "/admin/install.html#docker"
  },"67": {
    "doc": "Installation",
    "title": "AWS EC2",
    "content": "See standalone section . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#aws-ec2",
    "relUrl": "/admin/install.html#aws-ec2"
  },"68": {
    "doc": "Installation",
    "title": "AWS Elactic Beanstalk",
    "content": ". | Install AWS cli: https://aws.amazon.com/cli/ | Download Amazon version from downloads site: datamaker-aws-${VERSION}.zip | Configure: aws configure | Upload artefacts to S3 bucket aws s3 mb s3://elasticbeanstalk-datamaker aws s3 cp datamaker-aws-1.0.5-SNAPSHOT.zip s3://elasticbeanstalk-datamaker . | Create instance profile aws iam create-instance-profile --instance-profile-name aws-datamaker-ec2-role { \"InstanceProfile\": { \"Path\": \"/\", \"InstanceProfileName\": \"aws-datamaker-ec2-role\", \"InstanceProfileId\": \"AIPA557JKSZV2JRYHDS5D\", \"Arn\": \"arn:aws:iam::957730166379:instance-profile/aws-datamaker-ec2-role\", \"CreateDate\": \"2021-10-13T18:07:05+00:00\", \"Roles\": [] } } . | Copy the Arn value and paste in the next section under IamInstanceProfile option . | Create config file: options.json [ { \"Namespace\": \"aws:autoscaling:launchconfiguration\", \"OptionName\": \"IamInstanceProfile\", \"Value\": \"arn:aws:iam::957730166379:instance-profile/aws-elasticbeanstalk-ec2-role\" }, { \"Namespace\": \"aws:elasticbeanstalk:application:environment\", \"OptionName\": \"LOADER_PATH\", \"Value\": \"/home/webapp/conf/jar\" }, { \"Namespace\": \"aws:autoscaling:launchconfiguration\", \"OptionName\": \"InstanceType\", \"Value\": \"t2.large\" }, { \"Namespace\": \"aws:elasticbeanstalk:application:environment\", \"OptionName\": \"SPRING_PROFILES_ACTIVE\", \"Value\": \"amazon\" }, { \"Namespace\": \"aws:elasticbeanstalk:application:environment\", \"OptionName\": \"LOGS_PATH\", \"Value\": \"/home/webapp/datamaker/logs\" }, { \"Namespace\": \"aws:rds:dbinstance\", \"OptionName\": \"DBAllocatedStorage\", \"Value\": \"5\" }, { \"Namespace\": \"aws:rds:dbinstance\", \"OptionName\": \"HasCoupledDatabase\", \"Value\": \"true\" }, { \"Namespace\": \"aws:rds:dbinstance\", \"OptionName\": \"DBUser\", \"Value\": \"root\" }, { \"Namespace\": \"aws:rds:dbinstance\", \"OptionName\": \"DBPassword\", \"Value\": \"changeme\" }, { \"Namespace\": \"aws:rds:dbinstance\", \"OptionName\": \"DBDeletionPolicy\", \"Value\": \"Snapshot\" }, { \"Namespace\": \"aws:rds:dbinstance\", \"OptionName\": \"DBName\", \"Value\": \"datamaker\" } ] . | Create an elastic beanstalk aws elasticbeanstalk create-application --application-name datamaker --description \"Datamaker Testing Suite\" . | Create application version aws elasticbeanstalk create-application-version --application-name datamaker --version-label v1.0.5 --description datamaker-v1.0.5 --source-bundle S3Bucket=\"elasticbeanstalk-datamaker\",S3Key=\"datamaker-aws-1.0.5-SNAPSHOT.zip\" --auto-create-application . | Create an environment aws elasticbeanstalk create-environment --application-name datamaker --environment-name datamaker-prod --version-label v1.0.5 --solution-stack-name \"64bit Amazon Linux 2 v3.2.6 running Corretto 11\" --option-settings --option-settings file://options.json . | To enable authentication: see Cognito | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#aws-elactic-beanstalk",
    "relUrl": "/admin/install.html#aws-elactic-beanstalk"
  },"69": {
    "doc": "Installation",
    "title": "Azure",
    "content": ". | Install Azure cli: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli | Download Azure version from downloads site: datamaker-azure-${VERSION}.zip | Login az login | Create (or reuse) a Resource Group: az group create --name web-datamaker-westus-prod-01 --location westus | Create Azure MySql: https://docs.microsoft.com/en-us/azure/mysql/quickstart-create-mysql-server-database-using-azure-cli . | az mysql server create --resource-group web-datamaker-westus-prod-01 --name datamaker-mysql --location westus --admin-user root --admin-password &lt;server_admin_password&gt; --sku-name GP_Gen5_2 | mysql -h datamaker-mysql.mysql.database.azure.com -u root@datamaker-mysql -p | CREATE DATABASE datamaker; | . | Create a webapp: az webapp create -g web-datamaker-westus-prod-01 -p MyPlan -n datamaker | Deploy package: az webapp deploy --resource-group web-datamaker-westus-prod-01 --name datamaker --src-path datamaker-azure-1.0.5-SNAPSHOT.zip --type zip | Override configuration settings: az webapp config appsettings set -g web-datamaker-westus-prod-01 -n datamaker --settings SPRING_PROFILES_ACTIVE=azure SPRING_DATASOURCE_URL=jdbc:mysql://datamaker-server.mysql.database.azure.com:3306/datamaker SPRING_DATASOURCE_USERNAME=psuuzsetwp@datamaker SPRING_DATASOURCE_PASSWORD=changeme LOGS_PATH=/home/LogFiles/Application . | More info available here: https://docs.microsoft.com/en-us/azure/app-service/deploy-zip?tabs=cli | Open browser to: https://datamaker.azurewebsites.net/datamaker | Login using admin / changeme credentials | To enable authentication: see Azure AD | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#azure",
    "relUrl": "/admin/install.html#azure"
  },"70": {
    "doc": "Installation",
    "title": "Google App Engine",
    "content": ". | Install Google Cloud SDK: https://cloud.google.com/sdk/docs/install | Download Google version from downloads site: datamaker-gcp-${VERSION}.zip | Login gcloud auth login | Create an App Engine app within the current Google Cloud Project : gcloud app create --region=us-central | Deploy package: gcloud app deploy your-executable.jar | Open browser to: https://.appspot.com | Login using admin / changeme credentials | Monitor the logs: ~/google-cloud-sdk/bin/gcloud app logs tail -s default | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#google-app-engine",
    "relUrl": "/admin/install.html#google-app-engine"
  },"71": {
    "doc": "Installation",
    "title": "Tomcat",
    "content": ". | Download WAR version from downloads site: datamaker-${VERSION}.zip | Upload the war inside the directory: $CATALINA_HOME/webapps . | You can also use deploy using the Tomcat Manager (needs configuration) | . | Create setenv.sh in $CATALINA_HOME/bin | Override Spring configuration: JAVA_OPTS=\"$JAVA_OPTS -Dspring.profiles.active= -Dspring.config.location=classpath:/,file:/home/datamaker/conf/jar/\" SPRING_DATASOURCE_URL=jdbc:mysql://127.0.0.1:3306/datamaker SPRING_DATASOURCE_USERNAME=root SPRING_DATASOURCE_PASSWORD=changeme . | Restart Tomcat | Open up a browser and navigate to http:///datamaker | Login using admin / changeme credentials | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#tomcat",
    "relUrl": "/admin/install.html#tomcat"
  },"72": {
    "doc": "Installation",
    "title": "Kubernetes",
    "content": "Coming soon . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#kubernetes",
    "relUrl": "/admin/install.html#kubernetes"
  },"73": {
    "doc": "Installation",
    "title": "Cloud (Custom)",
    "content": "Coming soon . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#cloud-custom",
    "relUrl": "/admin/install.html#cloud-custom"
  },"74": {
    "doc": "Installation",
    "title": "Google Marketplace",
    "content": "Coming soon . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#google-marketplace",
    "relUrl": "/admin/install.html#google-marketplace"
  },"75": {
    "doc": "Installation",
    "title": "AWS Marketplace",
    "content": ". | Log into AWS Console | Navigate to EC2 service | Launch a new instance | Select AWS Marketplace | Search for Datamaker | Configure your instance and create it | See here for more info | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#aws-marketplace",
    "relUrl": "/admin/install.html#aws-marketplace"
  },"76": {
    "doc": "Installation",
    "title": "Azure Marketplace",
    "content": "Coming soon . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#azure-marketplace",
    "relUrl": "/admin/install.html#azure-marketplace"
  },"77": {
    "doc": "Installation",
    "title": "Snowflake Data Marketplace",
    "content": "Coming soon . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/install.html#snowflake-data-marketplace",
    "relUrl": "/admin/install.html#snowflake-data-marketplace"
  },"78": {
    "doc": "Jobs",
    "title": "Generate data jobs",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/jobs.html#generate-data-jobs",
    "relUrl": "/user/jobs.html#generate-data-jobs"
  },"79": {
    "doc": "Jobs",
    "title": "Table of contents",
    "content": ". | To view a list of all jobs: Jobs | To view a list of job executions | Create data generation job . | Settings | Datasets | Generator | Sinks | Last step | . | . To view a list of all jobs: Jobs . | Name | Description | Workspace | Last Run Status: possible values . | COMPLETED: finish successfully | RUNNING: job is started | CANCELLED: job was stopped during runtime | INIT: job is not started yet | FAILED: job run and halted with errors | . | . To view a list of job executions . Click on the History button . | State: see DataJobs | Start time | End time | Errors: contains error messages and exceptions if the job failed | Results: contains helpful messages | Replay: if the job is replayable, you can replay the job with the saved results as output. | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/jobs.html#table-of-contents",
    "relUrl": "/user/jobs.html#table-of-contents"
  },"80": {
    "doc": "Jobs",
    "title": "Create data generation job",
    "content": "There are 4 sections to configure a data generation job. Complete each section, so you can schedule or run the job. Settings . | Workspace: select one | Name | Description | Schedule type: . | Once: run one time only | Cron: quartz cron string (http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html). 6 digits format: 0 * * * * * | Random: start the job in a random window. Configure the minimum and maximum delays. | . | Maximum number of records: max records | Randomize number of records | Flush sink after each record creation: determine if the record is push to the sink after each creation | Use buffer: control if records are stored in temporary memory before being pushed to the sink. Can optimize processing time if sink is slow. Act as a back pressure control. | Buffer size: use in conjunction with Use buffer setting | Thread pool size: controls how many thread are spawn to generate records | Support job replay: flag that determine if job results will be persisted locally for future uses. That way you can replay the same records over and over in time. | Number of replay: how many job replays are kept. Old ones are erased automatically. | . Datasets . | Check which datasets will be part of the job | . Generator . | Select one generator from the list | Field options: each generator as its own specific configuration, see Generators | . Sinks . | Select one sink from the list | Prefill from existing configuration: if a Global sink was configured, you can use it here. | Sink options: each sink as its own specific configuration, see Sinks | . Last step . | Schedule job: save and run the job based on the schedule | Download file: run the job immediately and download the results | Show output: run the job immediately and show the results in the console | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/jobs.html#create-data-generation-job",
    "relUrl": "/user/jobs.html#create-data-generation-job"
  },"81": {
    "doc": "Jobs",
    "title": "Jobs",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/jobs.html",
    "relUrl": "/user/jobs.html"
  },"82": {
    "doc": "Release Notes",
    "title": "Version 1.0.7",
    "content": ". | Added missing documentation | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#version-107",
    "relUrl": "/notes/#version-107"
  },"83": {
    "doc": "Release Notes",
    "title": "Version 1.0.6",
    "content": ". | Added AMI in AWS Marketplace | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#version-106",
    "relUrl": "/notes/#version-106"
  },"84": {
    "doc": "Release Notes",
    "title": "Version 1.0.2",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#version-102",
    "relUrl": "/notes/#version-102"
  },"85": {
    "doc": "Release Notes",
    "title": "Bug Fixes",
    "content": ". | Fix CSV encoding | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#bug-fixes",
    "relUrl": "/notes/#bug-fixes"
  },"86": {
    "doc": "Release Notes",
    "title": "New Features",
    "content": ". | Added data job re-playable feature. You can re-generate the same data as previous jobs. | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#new-features",
    "relUrl": "/notes/#new-features"
  },"87": {
    "doc": "Release Notes",
    "title": "Improvements",
    "content": ". | Logger configuration file (logback.xml) | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#improvements",
    "relUrl": "/notes/#improvements"
  },"88": {
    "doc": "Release Notes",
    "title": "Version 1.0.1",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#version-101",
    "relUrl": "/notes/#version-101"
  },"89": {
    "doc": "Release Notes",
    "title": "New Features",
    "content": ". | Added new sinks . | Amazon Kinesis | Kafka | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#new-features-1",
    "relUrl": "/notes/#new-features-1"
  },"90": {
    "doc": "Release Notes",
    "title": "Version 1.0.0",
    "content": ". | Initial version | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/#version-100",
    "relUrl": "/notes/#version-100"
  },"91": {
    "doc": "Release Notes",
    "title": "Release Notes",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/notes/",
    "relUrl": "/notes/"
  },"92": {
    "doc": "Rest API",
    "title": "Rest API",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/rest_api.html",
    "relUrl": "/developer/rest_api.html"
  },"93": {
    "doc": "Rest API",
    "title": "Authentication",
    "content": "Grab session cookie and reuse it . curl --cookie cookie.txt --cookie-jar cookie.txt -d \"username=admin&amp;password=changeme\" -X POST http://localhost:8080/datamaker/login curl --cookie cookie.txt --cookie-jar cookie.txt http://localhost:8080/datamaker/api/user . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/rest_api.html#authentication",
    "relUrl": "/developer/rest_api.html#authentication"
  },"94": {
    "doc": "Rest API",
    "title": "Documentation",
    "content": "See swagger UI . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/rest_api.html#documentation",
    "relUrl": "/developer/rest_api.html#documentation"
  },"95": {
    "doc": "SDK",
    "title": "SDK",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/sdk.html",
    "relUrl": "/developer/sdk.html"
  },"96": {
    "doc": "SDK",
    "title": "Table of contents",
    "content": ". | How to extend functionalities | Processor . | Code sample | Base class | . | Sink . | Code sample | Interface class | . | Field . | Code sample | Base class | . | Generator . | Code sample | Interface class | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/sdk.html#table-of-contents",
    "relUrl": "/developer/sdk.html#table-of-contents"
  },"97": {
    "doc": "SDK",
    "title": "How to extend functionalities",
    "content": ". | Create basic Maven project | . mvn archetype:generate -DgroupId=ai.datamaker -DartifactId=demo-sdk -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false . | Make sure the library (jar) is installed in your local or remote repository (Nexus, Artefactory) | Add Datamaker dependency to pom.xml | . &lt;dependency&gt; &lt;groupId&gt;ai.datamaker&lt;/groupId&gt; &lt;artifactId&gt;service&lt;/artifactId&gt; &lt;version&gt;1.0.6&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; . | Build: mvn clean install | Make sure there are no compile errors | From there you can create custom components to enrich your Datamaker instance | When you are done developing, you can copy the target jar on the server running datamaker in the following path . | loader path: # APPLICATION FILES application.config.path=/tmp/datamaker/conf loader.path=${application.config.path}/jar . | . | Make sure your copy all dependencies required by your library | Restart the service | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/sdk.html#how-to-extend-functionalities",
    "relUrl": "/developer/sdk.html#how-to-extend-functionalities"
  },"98": {
    "doc": "SDK",
    "title": "Processor",
    "content": "The processors are mainly used to automatically create datasets from well-known file formats. For example, if we have a CSV file that we want to process, we can use the CsvProcessor. This processor will use the headers and values to create the corresponding dataset. A processor takes an InputStream and convert it to a dataset using the provided configuration. Minimally you need to implement these two methods: . public abstract Optional&lt;Dataset&gt; process(InputStream input, JobConfig config); . public abstract Set&lt;SupportedMediaType&gt; supportedTypes(); . Let say we have a custom format that we want to process. This file used a basic structure such as TLV (Tag-Length-Value) triplets. 1 byte = record type 4 bytes = record length followed by record content . Code sample . @Override public Optional&lt;Dataset&gt; process(InputStream input, JobConfig config) { Locale locale = getLocale(config); String datasetName = (String)config.getConfigProperty(INPUT_FILENAME_PROPERTY); Dataset dataset = new Dataset(datasetName,locale); try { int position = 0; while (true) { int type = input.read(); position += 1; byte[] lengthBuffer = new byte[4]; int r = input.read(lengthBuffer, 0, 4); position += 3; int length = ByteBuffer.wrap(lengthBuffer).getInt(); byte[] content = new byte[length]; int result = input.read(content, 0, length); position += length; if (type == 1) { IntegerField integerField = new IntegerField(new String(content), locale); dataset.addField(integerField); } else if (type == 2) { StringField stringField = new StringField(new String(content), locale); dataset.addField(stringField); } if (result == -1) { break; } } } catch (IOException e) { throw new DatasetSerializationException(\"invalid data\", e, dataset); } return Optional.of(dataset); } . Base class . package ca.breakpoints.datamaker.processor; import ca.breakpoints.datamaker.model.*; import ca.breakpoints.datamaker.model.field.Field; import ca.breakpoints.datamaker.service.FieldDetectorService; import java.io.InputStream; import java.util.*; import org.springframework.beans.factory.annotation.Autowired; /** * Process input source to generate dataset automatically. * Determine data type based on values. * Apply default rules. */ public abstract class DatasetProcessor implements Configurable { static final PropertyConfig LOCALE_PROPERTY = new PropertyConfig(Constants.LOCALE, \"Locale\", PropertyConfig.ValueType.STRING, Locale.ENGLISH.toLanguageTag(), Arrays.asList(Locale.ENGLISH.toLanguageTag(), Locale.FRENCH.toLanguageTag())); static final PropertyConfig INPUT_FILENAME_PROPERTY = new PropertyConfig(Constants.INPUT_FILENAME_KEY, \"Input filename\", PropertyConfig.ValueType.STRING, \"\", Collections.emptyList()); public Locale getLocale(JobConfig config) { return Locale.forLanguageTag((String) config.getConfigProperty(LOCALE_PROPERTY)); } @Autowired protected FieldDetectorService fieldDetectorService; public Optional&lt;Dataset&gt; process(InputStream input) { return process(input, JobConfig.EMPTY); } public abstract Optional&lt;Dataset&gt; process(InputStream input, JobConfig config); public abstract Set&lt;SupportedMediaType&gt; supportedTypes(); protected Optional&lt;Field&gt; detectField(String name, Collection&lt;Object&gt; values) { return Optional.empty(); } } . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/sdk.html#processor",
    "relUrl": "/developer/sdk.html#processor"
  },"99": {
    "doc": "SDK",
    "title": "Sink",
    "content": "A sink is the last link in the chain. It sends the data to one or multiple receivers. You must implement these methods: . boolean accept(FormatType type); . OutputStream getOutputStream(JobConfig config) throws Exception; . List&lt;PropertyConfig&gt; getConfigProperties(); . Code sample . package ca.breakpoints.datamaker.sink.base; import ca.breakpoints.datamaker.generator.FormatType; import ca.breakpoints.datamaker.model.DataOutputSinkType; import ca.breakpoints.datamaker.model.JobConfig; import ca.breakpoints.datamaker.model.PropertyConfig; import ca.breakpoints.datamaker.model.PropertyConfig.ValueType; import ca.breakpoints.datamaker.model.job.JobExecution; import ca.breakpoints.datamaker.sink.DataOutputSink; import com.google.common.collect.Lists; import lombok.extern.slf4j.Slf4j; import java.io.File; import java.io.FileOutputStream; import java.io.OutputStream; import java.util.Collections; import java.util.List; /** * Will generate a downloadable file. */ @Slf4j @DataOutputSinkType(compressed = true, encrypted = true) public class FileOutputSink implements DataOutputSink { public static final PropertyConfig FILE_OUTPUT_PATH_PROPERTY = new PropertyConfig( \"file.sink.output.filename\", \"Output file path\", ValueType.EXPRESSION, \"\\\"/tmp/\\\" + #dataset.name + \\\"-\\\" + T(java.lang.System).currentTimeMillis() + \\\".\\\" + #dataJob.generator.dataType.name().toLowerCase()\", Collections.emptyList()); @Override public boolean accept(FormatType type) { return true; } @Override public List&lt;PropertyConfig&gt; getConfigProperties() { return Lists.newArrayList(FILE_OUTPUT_PATH_PROPERTY); } public OutputStream getOutputStream(JobConfig config) throws Exception { JobExecution jobExecution = config.getJobExecution(); String path = (String) config.getConfigProperty(FILE_OUTPUT_PATH_PROPERTY); jobExecution.getResults().add(path); return new FileOutputStream(new File(path)); } } . Interface class . public interface DataOutputSink extends Configurable { ExpressionParser EXPRESSION_PARSER = new SpelExpressionParser(); boolean accept(FormatType type); default boolean flushable() { return true; } default OutputStream getOutputStream() throws Exception { return getOutputStream(new JobConfig()); } OutputStream getOutputStream(JobConfig config) throws Exception; default Object parseExpression(String expression, JobConfig config) { EvaluationContext evaluationContext = new StandardEvaluationContext(); evaluationContext.setVariable(\"dataset\", config.getDataset()); evaluationContext.setVariable(\"dataJob\", config.getGenerateDataJob()); evaluationContext.setVariable(\"jobExecution\", config.getJobExecution()); Expression exp = EXPRESSION_PARSER.parseExpression(expression); //return exp.getValue(); return exp.getValue(evaluationContext); } default List&lt;List&lt;SimpleFieldValue&gt;&gt; getRecords(InputStream inputStream) throws Exception { try (ObjectInputStream in = new ObjectInputStream(inputStream)) { return (List&lt;List&lt;SimpleFieldValue&gt;&gt;) in.readObject(); } } default List&lt;SimpleFieldValue&gt; getRecord(InputStream inputStream) throws Exception { try (ObjectInputStream in = new ObjectInputStream(inputStream)) { return (List&lt;SimpleFieldValue&gt;) in.readObject(); } } default OutputStream encryptCompressStream(JobConfig config, OutputStream outputStream) throws Exception { String compressionFormat = (String) config.getConfigProperty(CompressFilter.COMPRESSION_FORMAT); String encryptionAlgorithm = (String) config.getConfigProperty(EncryptFilter.ENCRYPTION_ALGORITHM); if (\"NONE\".equals(compressionFormat) &amp;&amp; \"NONE\".equals(encryptionAlgorithm)) { return outputStream; } else if (\"NONE\".equals(compressionFormat)) { return EncryptFilter.encryptStream(config, outputStream); } else if (\"NONE\".equals(encryptionAlgorithm)) { return CompressFilter.getCompressedStream(config, outputStream); } OutputStream compressedStream = CompressFilter.getCompressedStream(config, outputStream); return new OutputStream() { final ByteArrayOutputStream baos = new ByteArrayOutputStream(); final OutputStream stream = EncryptFilter.encryptStream(config, baos); @Override public void write(int b) throws IOException { stream.write(b); } @Override public void write(byte[] b) throws IOException { stream.write(b); } @Override public void write(byte[] b, int off, int len) throws IOException { stream.write(b, off, len); } @Override public void flush() throws IOException { stream.flush(); } @Override public void close() throws IOException { stream.flush(); stream.close(); compressedStream.write(baos.toByteArray()); compressedStream.close(); } }; } } . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/sdk.html#sink",
    "relUrl": "/developer/sdk.html#sink"
  },"100": {
    "doc": "SDK",
    "title": "Field",
    "content": "Dataset contains fields. A field generates a value based on it’s type. You must implement this method: . protected abstract V generateData(); . List&lt;PropertyConfig&gt; getConfigProperties(); . Code sample . package ca.breakpoints.datamaker.model.field.type; import ca.breakpoints.datamaker.model.PropertyConfig; import ca.breakpoints.datamaker.model.field.Field; import ca.breakpoints.datamaker.model.field.FieldGroup; import ca.breakpoints.datamaker.model.field.FieldType; import com.google.common.collect.Lists; import java.util.Arrays; import java.util.List; import java.util.Locale; import java.util.concurrent.ThreadLocalRandom; import javax.persistence.Entity; import lombok.NoArgsConstructor; import org.hibernate.search.annotations.Indexed; @NoArgsConstructor @Entity @Indexed @FieldType(description = \"Age, Integer [1-100]\", localizationKey = \"field.group.age\", group = FieldGroup.IDENTITY) public class AgeField extends Field&lt;Integer&gt; { static final PropertyConfig MINIMUM_AGE_PROPERTY = new PropertyConfig(\"field.age.minAge\", \"Mininum age\", PropertyConfig.ValueType.NUMERIC, 1, Arrays.asList(1, 125)); static final PropertyConfig MAXIMUM_AGE_PROPERTY = new PropertyConfig(\"field.age.maxAge\", \"Maximum age\", PropertyConfig.ValueType.NUMERIC, 125, Arrays.asList(1, 125)); public AgeField(String name, Locale locale) { super(name, locale); } @Override public Integer generateData() { int minAge = (int) config.getConfigProperty(MINIMUM_AGE_PROPERTY); int maxAge = (int) config.getConfigProperty(MAXIMUM_AGE_PROPERTY); if (maxAge &lt; minAge) { throw new IllegalArgumentException(\"Maximum age should be greather than minimum age\"); } return ThreadLocalRandom.current().nextInt(minAge, maxAge + 1); } @Override public List&lt;PropertyConfig&gt; getConfigProperties() { List&lt;PropertyConfig&gt; fieldConfigProperties = Lists.newArrayList(); fieldConfigProperties.add(MINIMUM_AGE_PROPERTY); fieldConfigProperties.add(MAXIMUM_AGE_PROPERTY); return fieldConfigProperties; } } . Base class . package ca.breakpoints.datamaker.model.field; import ca.breakpoints.datamaker.model.Configurable; import ca.breakpoints.datamaker.model.Dataset; import ca.breakpoints.datamaker.model.PropertyConfig; import ca.breakpoints.datamaker.model.Searchable; import ca.breakpoints.datamaker.model.field.formatter.FieldFormatter; import ca.breakpoints.datamaker.model.field.type.NullField; import ca.breakpoints.datamaker.repository.FieldRepository; import ca.breakpoints.datamaker.service.BeanService; import ca.breakpoints.datamaker.service.EncryptionService; import ca.breakpoints.datamaker.service.FieldService; import ca.breakpoints.datamaker.utils.FakerUtils; import com.fasterxml.jackson.annotation.JsonProperty; import com.github.javafaker.Faker; import com.google.common.annotations.VisibleForTesting; import com.google.common.collect.Lists; import lombok.Data; import lombok.EqualsAndHashCode; import lombok.ToString; import lombok.extern.slf4j.Slf4j; import org.apache.commons.collections.CollectionUtils; import org.apache.commons.lang.StringUtils; import javax.persistence.Access; import javax.persistence.AccessType; import javax.persistence.Column; import javax.persistence.Entity; import javax.persistence.FetchType; import javax.persistence.GeneratedValue; import javax.persistence.GenerationType; import javax.persistence.Id; import javax.persistence.JoinColumn; import javax.persistence.Lob; import javax.persistence.ManyToOne; import javax.persistence.PostLoad; import javax.persistence.PrePersist; import javax.persistence.PreUpdate; import javax.persistence.Transient; import javax.validation.constraints.NotBlank; import javax.validation.constraints.NotNull; import java.io.Serializable; import java.lang.reflect.ParameterizedType; import java.lang.reflect.Type; import java.util.Collection; import java.util.Collections; import java.util.Date; import java.util.Locale; import java.util.UUID; import java.util.concurrent.ThreadLocalRandom; import java.util.stream.Collectors; @Data @Entity @Slf4j public abstract class Field&lt;V&gt; implements Configurable, Serializable, Searchable { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) protected Long id; @org.hibernate.search.annotations.Field @Column(nullable = false, unique = true) private UUID externalId; // Leave blank to generate random value @Lob protected FieldConfig config = new FieldConfig(); @ManyToOne(fetch = FetchType.LAZY) @JoinColumn(name = \"dataset_id\") @ToString.Exclude @EqualsAndHashCode.Exclude private Dataset dataset; // Defaults to workspace locale // The locale is used to format the data // Example: currency or even first name // If user select locale=FR and first name, the service will generate French's first name (Jean-Claude instead of Kevin) @NotNull private Locale locale = Locale.getDefault(); // Column name, header @NotBlank @org.hibernate.search.annotations.Field private String name; @org.hibernate.search.annotations.Field private String description; private Boolean isNullable = false; private String nullValue = \"\"; @Transient protected FieldFormatter&lt;V&gt; formatter; @Access(AccessType.PROPERTY) @org.hibernate.search.annotations.Field private String formatterClassName; private Integer position = 0; // Use for XML formatting or else... private Boolean isAttribute = false; private Boolean isAlias = false; protected Boolean isPrimaryKey = false; private Boolean isNested = false; private Date dateCreated = new Date(); @JsonProperty(access = JsonProperty.Access.READ_ONLY) private Date dateModified; @Transient private Class&lt;V&gt; objectType; @Transient @org.hibernate.search.annotations.Field private String className = getClass().getSimpleName(); @Transient protected transient Faker faker; @SuppressWarnings(\"unchecked\") public Field() { this.externalId = UUID.randomUUID(); this.faker = FakerUtils.getFakerForLocale(locale); if (getClass().getGenericSuperclass() instanceof ParameterizedType) { Type pt = ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0]; if (pt instanceof ParameterizedType) { this.objectType = (Class&lt;V&gt;) ((ParameterizedType) pt).getRawType(); } else { this.objectType = (Class&lt;V&gt;) pt; } } else { this.objectType = (Class&lt;V&gt;) Class.class; } this.config.setField(this); } @SuppressWarnings(\"unchecked\") public Field(String name, Locale locale) { this.name = name; this.locale = locale; this.externalId = UUID.randomUUID(); this.faker = FakerUtils.getFakerForLocale(locale); if (getClass().getGenericSuperclass() instanceof ParameterizedType) { Type pt = ((ParameterizedType) getClass().getGenericSuperclass()).getActualTypeArguments()[0]; if (pt instanceof ParameterizedType) { this.objectType = (Class&lt;V&gt;) ((ParameterizedType) pt).getRawType(); } else { this.objectType = (Class&lt;V&gt;) pt; } } else { this.objectType = (Class&lt;V&gt;) Class.class; } this.config.setField(this); } protected abstract V generateData(); public Object getData() { if (isNullable &amp;&amp; dataset.getNullablePercentLimit() &gt; 0.0f) { // Generate random null 10% of time (max) if (ThreadLocalRandom.current().nextFloat() &lt;= dataset.getNullablePercentLimit()) { return null; } } V value = generateData(); if (formatter != null) { return formatter.format(value, config); } return value; } public void setLocale(Locale locale) { this.locale = locale; this.faker = FakerUtils.getFakerForLocale(locale); } public String getFormatterClassName() { return formatterClassName; } public void setFormatterClassName(String formatterClass) { if (StringUtils.isNotBlank(formatterClass)) { try { this.formatterClassName = formatterClass; this.formatter = (FieldFormatter) Class.forName(formatterClass).getDeclaredConstructor().newInstance(); } catch (Exception e) { throw new IllegalStateException(e.getMessage(), e); } } } @VisibleForTesting protected Field getFieldFor(String externalId) { FieldService fieldService = BeanService.getBean(FieldService.class); return fieldService.getFieldFor(externalId); } public Collection&lt;String&gt; getTags() { return Collections.emptyList(); } protected NullField createNull() { NullField nullField = new NullField(\"null\", locale); nullField.setDataset(dataset); return nullField; } } . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/sdk.html#field",
    "relUrl": "/developer/sdk.html#field"
  },"101": {
    "doc": "SDK",
    "title": "Generator",
    "content": "A generator implementation will generate data based on a dataset definition. You need to implement the following method: . void generate(Dataset dataset, OutputStream outputStream) throws Exception; . Code sample . package ca.breakpoints.datamaker.generator; import ca.breakpoints.datamaker.model.Dataset; import ca.breakpoints.datamaker.model.JobConfig; import ca.breakpoints.datamaker.model.PropertyConfig; import ca.breakpoints.datamaker.model.PropertyConfig.ValueType; import com.google.common.collect.Lists; import java.io.IOException; import java.io.OutputStream; import java.util.Collections; import java.util.List; public class TextGenerator implements DataGenerator { static final PropertyConfig ELEMENT_SEPARATOR = new PropertyConfig(\"text.generator.element.separator\", \"Element separator\", PropertyConfig.ValueType.STRING, \"\", Collections.emptyList()); static final PropertyConfig KEY_VALUE_SEPARATOR = new PropertyConfig(\"text.generator.key.value.separator\", \"Key value separator\", PropertyConfig.ValueType.STRING, \"=\", Collections.emptyList()); static final PropertyConfig OUTPUT_KEYS = new PropertyConfig(\"text.generator.output.keys\", \"Output keys\", ValueType.BOOLEAN, \"false\", Collections.emptyList()); @Override public void generate(Dataset dataset, OutputStream outputStream) throws Exception { generate(dataset, outputStream, JobConfig.EMPTY); } @Override public void generate(Dataset dataset, OutputStream outputStream, JobConfig config) throws Exception { dataset.processAllValues(fv -&gt; { fv.forEach(value -&gt; { try { if (Boolean.parseBoolean(config.getConfigProperty(OUTPUT_KEYS).toString())) { outputStream.write(value.getField().getName().getBytes()); outputStream.write(config.getConfigProperty(KEY_VALUE_SEPARATOR).toString().getBytes()); } outputStream.write(value.getValue().toString().getBytes()); outputStream.write(config.getConfigProperty(ELEMENT_SEPARATOR).toString().getBytes()); } catch (IOException e) { throw new IllegalStateException(e); } } ); }); } @Override public FormatType getDataType() { return FormatType.TEXT; } @Override public List&lt;PropertyConfig&gt; getConfigProperties() { return Lists.newArrayList(ELEMENT_SEPARATOR, KEY_VALUE_SEPARATOR, OUTPUT_KEYS); } } . Interface class . /** * Generate data based on a dataset. */ public interface DataGenerator extends Configurable { ExpressionParser EXPRESSION_PARSER = new SpelExpressionParser(); void generate(Dataset dataset, OutputStream outputStream) throws Exception; default void generate(Dataset dataset, OutputStream outputStream, JobConfig config) throws Exception { generate(dataset, outputStream); } FormatType getDataType(); default Object parseExpression(String expression, Dataset dataset) { EvaluationContext evaluationContext = new StandardEvaluationContext(); evaluationContext.setVariable(\"dataset\", dataset); // evaluationContext.setVariable(\"dataJob\", config.getGenerateDataJob()); // evaluationContext.setVariable(\"jobExecution\", config.getJobExecution()); Expression exp = EXPRESSION_PARSER.parseExpression(expression); //return exp.getValue(); return exp.getValue(evaluationContext); } } . ",
    "url": "https://www.datamaker.ai/datamaker/docs/developer/sdk.html#generator",
    "relUrl": "/developer/sdk.html#generator"
  },"102": {
    "doc": "Security",
    "title": "Security",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/security.html",
    "relUrl": "/admin/security.html"
  },"103": {
    "doc": "Security",
    "title": "Generate secure password (hashed)",
    "content": "To overwrite a password you can use this command: . htpasswd -bnBC 10 \"\" password | tr -d ':\\n' | sed 's/$2y/$2a/' . admin.password=$2a$10$loR4oKdMPHdpQwVyem5TMu3vR3ktQdGHmQgvKeP3VKiWkim5OvbUa . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/security.html#generate-secure-password-hashed",
    "relUrl": "/admin/security.html#generate-secure-password-hashed"
  },"104": {
    "doc": "Security",
    "title": "Encryption key",
    "content": "The default encryption key is changeme. You can override it using environment variable, java options or application.properties. encryption.secret.key=changeme encryption.salt=changeme . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/security.html#encryption-key",
    "relUrl": "/admin/security.html#encryption-key"
  },"105": {
    "doc": "Security",
    "title": "SSL",
    "content": "To generate a self-signed certificate (or use your own): $ keytool -genkeypair -keyalg RSA -keysize 2048 -storetype PKCS12 -keystore cert.p12 -validity 365 . Uncomment these settings in application.properties: . # SSL server.port=8443 server.ssl.key-store=/home/datamaker/cert.p12 server.ssl.key-store-password=123456 # JKS or PKCS12 server.ssl.keyStoreType=PKCS12 # Spring Security security.require-ssl=true . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/security.html#ssl",
    "relUrl": "/admin/security.html#ssl"
  },"106": {
    "doc": "Sinks",
    "title": "Sinks",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html",
    "relUrl": "/user/sinks.html"
  },"107": {
    "doc": "Sinks",
    "title": "Table of contents",
    "content": ". | Global Sink Configurations . | To view a list of all sinks: Sinks | To create a new sink configuration: Edit Sink | . | Supported Sinks . | AMAZON . | Amazon Glacier | Amazon Kinesis | Amazon S3 | . | AZURE . | Azure Blob Storage | Azure Datalake | Azure Event Hub | Azure Storage Queue | . | BASE . | Cifs | Copy | Email | File | Ftp | Http | Jdbc | Jms | Log | Proxy | Sftp | Socket | String | Web Dav | . | ELASTIC . | Elastic Search | . | GOOGLE . | Google Cloud Storage | Google Pub Sub | . | HADOOP . | Hdfs | Knox Ouput Sink | Web Hdfs | . | IBM . | Ibm Cloud Object Storage | . | KAFKA . | Kafka | . | SINK . | Object | . | SOLR . | Solr | . | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#table-of-contents",
    "relUrl": "/user/sinks.html#table-of-contents"
  },"108": {
    "doc": "Sinks",
    "title": "Global Sink Configurations",
    "content": "To view a list of all sinks: Sinks . To create a new sink configuration: Edit Sink . | Name | Worskspace | Select sink: Select from the supported list | Configure additional parameters for the sink | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#global-sink-configurations",
    "relUrl": "/user/sinks.html#global-sink-configurations"
  },"109": {
    "doc": "Sinks",
    "title": "Supported Sinks",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#supported-sinks",
    "relUrl": "/user/sinks.html#supported-sinks"
  },"110": {
    "doc": "Sinks",
    "title": "AMAZON",
    "content": "Amazon Glacier . Description: . Class: ca.breakpoints.datamaker.sink.amazon.AmazonGlacierVaultOutputSink . Configuration: . | AWS Credentials access key id . | Type: STRING | Default value: | Possible values: | . | AWS Credentials secret access key . | Type: PASSWORD | Default value: | Possible values: | . | AWS Region . | Type: STRING | Default value: us-east-1 | Possible values: ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global | . | . Amazon Kinesis . Description: . Class: ca.breakpoints.datamaker.sink.amazon.AmazonKinesisOutputSink . Configuration: . | Stream name (ARN) . | Type: STRING | Default value: | Possible values: | . | Partition key . | Type: EXPRESSION | Default value: #dataset.name | Possible values: | . | AWS Credentials access key id . | Type: STRING | Default value: | Possible values: | . | AWS Credentials secret access key . | Type: PASSWORD | Default value: | Possible values: | . | AWS Region . | Type: STRING | Default value: us-east-1 | Possible values: ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global | . | . Amazon S3 . Description: . Class: ca.breakpoints.datamaker.sink.amazon.AmazonS3OutputSink . Configuration: . | S3 Bucket name . | Type: STRING | Default value: | Possible values: | . | S3 Bucket creation . | Type: BOOLEAN | Default value: False | Possible values: | . | S3 Object key . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | AWS Credentials access key id . | Type: STRING | Default value: | Possible values: | . | AWS Credentials secret access key . | Type: PASSWORD | Default value: | Possible values: | . | AWS Region . | Type: STRING | Default value: us-east-1 | Possible values: ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#amazon",
    "relUrl": "/user/sinks.html#amazon"
  },"111": {
    "doc": "Sinks",
    "title": "AZURE",
    "content": "Azure Blob Storage . Description: . Class: ca.breakpoints.datamaker.sink.azure.AzureBlobStorageOutputSink . Configuration: . | Container name . | Type: STRING | Default value: | Possible values: | . | Blob name . | Type: EXPRESSION | Default value: | Possible values: | . | Blob type . | Type: STRING | Default value: BLOCK | Possible values: BLOCK, APPEND, PAGE, SNAPSHOT | . | Page range start . | Type: NUMERIC | Default value: 0 | Possible values: | . | Page range end . | Type: NUMERIC | Default value: 0 | Possible values: | . | Snapshot ID . | Type: EXPRESSION | Default value: | Possible values: | . | Storage SAS TOKEN . | Type: PASSWORD | Default value: | Possible values: | . | Storage account name . | Type: STRING | Default value: | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Storage account key . | Type: PASSWORD | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Azure Datalake . Description: . Class: ca.breakpoints.datamaker.sink.azure.AzureDatalakeOutputSink . Configuration: . | Filesystem name . | Type: STRING | Default value: | Possible values: | . | File name . | Type: EXPRESSION | Default value: | Possible values: | . | Storage buffer . | Type: NUMERIC | Default value: 1048576 | Possible values: | . | Storage SAS TOKEN . | Type: PASSWORD | Default value: | Possible values: | . | Storage account name . | Type: STRING | Default value: | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Storage account key . | Type: PASSWORD | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Azure Event Hub . Description: . Class: ca.breakpoints.datamaker.sink.azure.AzureEventHubOutputSink . Configuration: . | Connection string . | Type: PASSWORD | Default value: | Possible values: | . | Batch size (bytes) . | Type: NUMERIC | Default value: 1024 | Possible values: | . | Batch duration (seconds) . | Type: NUMERIC | Default value: 1 | Possible values: | . | . Azure Storage Queue . Description: . Class: ca.breakpoints.datamaker.sink.azure.AzureStorageQueueOutputSink . Configuration: . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#azure",
    "relUrl": "/user/sinks.html#azure"
  },"112": {
    "doc": "Sinks",
    "title": "BASE",
    "content": "Cifs . Description: . Class: ca.breakpoints.datamaker.sink.base.CifsOutputSink . Configuration: . | Hostname . | Type: STRING | Default value: remote | Possible values: | . | Port . | Type: NUMERIC | Default value: 139 | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: “/” + #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Copy . Description: . Class: ca.breakpoints.datamaker.sink.base.CopyOutputSink . Configuration: . Email . Description: . Class: ca.breakpoints.datamaker.sink.base.EmailOutputSink . Configuration: . | Data output format . | Type: STRING | Default value: PLAIN_TEXT | Possible values: ATTACHMENT, PLAIN_TEXT, HTML | . | Sender address (From) . | Type: STRING | Default value: | Possible values: | . | Recipient address (To) . | Type: STRING | Default value: | Possible values: | . | SMTP host . | Type: STRING | Default value: localhost | Possible values: | . | SMTP port . | Type: NUMERIC | Default value: 25 | Possible values: | . | Subject line . | Type: STRING | Default value: | Possible values: | . | Message part in case of attachment . | Type: STRING | Default value: | Possible values: | . | File attachment name . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Secured authentication . | Type: STRING | Default value: | Possible values: , SSL, TLS | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . File . Description: . Class: ca.breakpoints.datamaker.sink.base.FileOutputSink . Configuration: . | Output file path . | Type: EXPRESSION | Default value: “/tmp/” + #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Ftp . Description: . Class: ca.breakpoints.datamaker.sink.base.FtpOutputSink . Configuration: . | Hostname . | Type: STRING | Default value: remote | Possible values: | . | Port . | Type: NUMERIC | Default value: 22 | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Http . Description: . Class: ca.breakpoints.datamaker.sink.base.HttpOutputSink . Configuration: . | Endpoint URL . | Type: STRING | Default value: | Possible values: | . | Method . | Type: STRING | Default value: POST | Possible values: PATCH, PUT, POST | . | Payload . | Type: STRING | Default value: BODY | Possible values: BODY, MULTIPART | . | Content type . | Type: EXPRESSION | Default value: | Possible values: | . | Authentication method . | Type: STRING | Default value: NONE | Possible values: BASIC, KERBEROS, NONE, SPNEGO | . | Http query names . | Type: LIST | Default value: [] | Possible values: | . | Http query values (support expression) . | Type: LIST | Default value: [] | Possible values: | . | Http header names . | Type: LIST | Default value: [] | Possible values: | . | Http header values (support expression) . | Type: LIST | Default value: [] | Possible values: | . | Form parameter names . | Type: LIST | Default value: [] | Possible values: | . | Form parameter values (support expression) . | Type: LIST | Default value: [] | Possible values: | . | Form parameter types (binary or text) . | Type: LIST | Default value: [] | Possible values: | . | Username . | Type: STRING | Default value: elastic | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Kerberos principal . | Type: STRING | Default value: | Possible values: | . | Kerberos keytab . | Type: STRING | Default value: | Possible values: | . | Truststore filename . | Type: STRING | Default value: | Possible values: | . | Truststore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Keystore filename . | Type: STRING | Default value: | Possible values: | . | Keystore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | . Jdbc . Description: . Class: ca.breakpoints.datamaker.sink.base.JdbcOutputSink . Configuration: . | JDBC driver class name . | Type: STRING | Default value: | Possible values: | . | Connection URL . | Type: STRING | Default value: | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | . Jms . Description: . Class: ca.breakpoints.datamaker.sink.base.JmsOutputSink . Configuration: . | A fully qualified name of the JMS ConnectionFactory implementation class (i.e., org.apache.activemq.ActiveMQConnectionFactory) . | Type: STRING | Default value: | Possible values: | . | Topic name . | Type: STRING | Default value: | Possible values: | . | Destination name (ex: topicName, queueName) . | Type: STRING | Default value: | Possible values: | . | Destination type . | Type: STRING | Default value: TOPIC | Possible values: QUEUE, TOPIC | . | Message ID . | Type: EXPRESSION | Default value: | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Truststore filename . | Type: STRING | Default value: | Possible values: | . | Truststore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Keystore filename . | Type: STRING | Default value: | Possible values: | . | Keystore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | . Log . Description: . Class: ca.breakpoints.datamaker.sink.base.LogOutputSink . Configuration: . | Prefix . | Type: STRING | Default value: DATA: | Possible values: | . | . Proxy . Description: . Class: ca.breakpoints.datamaker.sink.base.ProxyOutputSink . Configuration: . Sftp . Description: . Class: ca.breakpoints.datamaker.sink.base.SftpOutputSink . Configuration: . | Hostname . | Type: STRING | Default value: remote | Possible values: | . | Port . | Type: NUMERIC | Default value: 139 | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Use User’s home as path root . | Type: BOOLEAN | Default value: | Possible values: | . | Private key path . | Type: STRING | Default value: | Possible values: | . | Passphrase . | Type: PASSWORD | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Socket . Description: . Class: ca.breakpoints.datamaker.sink.base.SocketOutputSink . Configuration: . | Port number . | Type: NUMERIC | Default value: 0 | Possible values: 0, 65535 | . | Hostname . | Type: STRING | Default value: localhost | Possible values: | . | Socket protocol . | Type: STRING | Default value: TCP | Possible values: TCP, UDP | . | Header message . | Type: STRING | Default value: | Possible values: | . | Footer message . | Type: STRING | Default value: | Possible values: | . | . String . Description: . Class: ca.breakpoints.datamaker.sink.base.StringOutputSink . Configuration: . Web Dav . Description: . Class: ca.breakpoints.datamaker.sink.base.WebDavOutputSink . Configuration: . | Hostname . | Type: STRING | Default value: remote | Possible values: | . | Port . | Type: NUMERIC | Default value: 80 | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: “/” + #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Username . | Type: STRING | Default value: | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#base",
    "relUrl": "/user/sinks.html#base"
  },"113": {
    "doc": "Sinks",
    "title": "ELASTIC",
    "content": "Elastic Search . Description: . Class: ca.breakpoints.datamaker.sink.elastic.ElasticSearchOutputSink . Configuration: . | Endpoints (scheme:host:port) . | Type: LIST | Default value: [‘http:localhost:9200’] | Possible values: | . | Index name . | Type: STRING | Default value: | Possible values: | . | Username . | Type: STRING | Default value: elastic | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Retry count . | Type: NUMERIC | Default value: 5 | Possible values: | . | Truststore filename . | Type: STRING | Default value: | Possible values: | . | Truststore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Keystore filename . | Type: STRING | Default value: | Possible values: | . | Keystore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#elastic",
    "relUrl": "/user/sinks.html#elastic"
  },"114": {
    "doc": "Sinks",
    "title": "GOOGLE",
    "content": "Google Cloud Storage . Description: . Class: ca.breakpoints.datamaker.sink.google.GoogleCloudStorageOutputSink . Configuration: . | Container name . | Type: STRING | Default value: | Possible values: | . | Object name . | Type: EXPRESSION | Default value: | Possible values: | . | Project ID . | Type: STRING | Default value: | Possible values: | . | Service account JSON key . | Type: STRING | Default value: {} | Possible values: | . | OAuth token value . | Type: STRING | Default value: | Possible values: | . | Authentication method . | Type: STRING | Default value: SERVICE_ACCOUNT | Possible values: OAUTH_TOKEN, PLATFORM, SERVICE_ACCOUNT | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Google Pub Sub . Description: . Class: ca.breakpoints.datamaker.sink.google.GooglePubSubOutputSink . Configuration: . | Topic name . | Type: STRING | Default value: | Possible values: | . | Project ID . | Type: STRING | Default value: | Possible values: | . | Authentication method . | Type: STRING | Default value: SERVICE_ACCOUNT | Possible values: OAUTH_TOKEN, PLATFORM, SERVICE_ACCOUNT | . | Service account JSON key . | Type: STRING | Default value: {} | Possible values: | . | OAuth token value . | Type: STRING | Default value: | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#google",
    "relUrl": "/user/sinks.html#google"
  },"115": {
    "doc": "Sinks",
    "title": "HADOOP",
    "content": "Hdfs . Description: . Class: ca.breakpoints.datamaker.sink.hadoop.HdfsOutputSink . Configuration: . | A file or comma separated list of files which contains the Hadoop file system configuration. Without this, Hadoop will search the classpath for a ‘core-site.xml’ and ‘hdfs-site.xml’ file or will revert to a default configuration. | Type: STRING | Default value: | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: ‘/tmp’ | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Kerberos principal . | Type: STRING | Default value: | Possible values: | . | Kerberos keytab . | Type: STRING | Default value: | Possible values: | . | Kerberized cluster . | Type: BOOLEAN | Default value: False | Possible values: | . | Name node . | Type: STRING | Default value: OVERWRITE | Possible values: APPEND, FAILED, OVERWRITE | . | Size of each block as written to HDFS . | Type: NUMERIC | Default value: 33554432 | Possible values: | . | Amount of memory to use to buffer file contents during IO . | Type: NUMERIC | Default value: 4096 | Possible values: | . | Number of times that HDFS will replicate each file . | Type: NUMERIC | Default value: 1 | Possible values: | . | A umask represented as an octal number which determines the permissions of files written to HDFS . | Type: NUMERIC | Default value: 18 | Possible values: | . | Changes the owner of the HDFS file to this value after it is written . | Type: STRING | Default value: | Possible values: | . | Changes the group of the HDFS file to this value after it is written . | Type: STRING | Default value: | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Knox Ouput Sink . Description: . Class: ca.breakpoints.datamaker.sink.hadoop.KnoxOuputSink . Configuration: . | Knox Endpoint URL . | Type: STRING | Default value: | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: “/tmp” | Possible values: | . | Append mode . | Type: BOOLEAN | Default value: False | Possible values: | . | Username . | Type: STRING | Default value: elastic | Possible values: | . | Password . | Type: PASSWORD | Default value: | Possible values: | . | Truststore filename . | Type: STRING | Default value: | Possible values: | . | Truststore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Keystore filename . | Type: STRING | Default value: | Possible values: | . | Keystore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . Web Hdfs . Description: . Class: ca.breakpoints.datamaker.sink.hadoop.WebHdfsOutputSink . Configuration: . | Endpoint URL . | Type: STRING | Default value: | Possible values: | . | Output file path . | Type: EXPRESSION | Default value: “/tmp” | Possible values: | . | Append mode . | Type: BOOLEAN | Default value: False | Possible values: | . | Kerberized cluster . | Type: BOOLEAN | Default value: False | Possible values: | . | Kerberos principal . | Type: STRING | Default value: | Possible values: | . | Kerberos keytab . | Type: STRING | Default value: | Possible values: | . | Truststore filename . | Type: STRING | Default value: | Possible values: | . | Truststore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Keystore filename . | Type: STRING | Default value: | Possible values: | . | Keystore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Compression format . | Type: STRING | Default value: NONE | Possible values: BZIP2, DEFLATE, GZIP, JAR, NONE, SNAPPY, TAR, TAR_BZIP2, TAR_GZIP, TGZ, ZIP | . | Original filename . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | Encryption algorithm . | Type: STRING | Default value: NONE | Possible values: BCRYPT, NONE, PGP | . | PGP public key path . | Type: STRING | Default value: | Possible values: | . | PGP armored . | Type: BOOLEAN | Default value: False | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#hadoop",
    "relUrl": "/user/sinks.html#hadoop"
  },"116": {
    "doc": "Sinks",
    "title": "IBM",
    "content": "Ibm Cloud Object Storage . Description: . Class: ca.breakpoints.datamaker.sink.ibm.IbmCloudObjectStorageOutputSink . Configuration: . | S3 Bucket name . | Type: STRING | Default value: | Possible values: | . | S3 Bucket creation . | Type: BOOLEAN | Default value: False | Possible values: | . | S3 Object key . | Type: EXPRESSION | Default value: #dataset.name + “-“ + T(java.lang.System).currentTimeMillis() + “.” + #dataJob.generator.dataType.name().toLowerCase() | Possible values: | . | AWS Credentials access key id . | Type: STRING | Default value: | Possible values: | . | AWS Credentials secret access key . | Type: PASSWORD | Default value: | Possible values: | . | AWS Region . | Type: STRING | Default value: us-east-1 | Possible values: ap-south-1, eu-south-1, us-gov-east-1, ca-central-1, eu-central-1, us-west-1, us-west-2, af-south-1, eu-north-1, eu-west-3, eu-west-2, eu-west-1, ap-northeast-3, ap-northeast-2, ap-northeast-1, me-south-1, sa-east-1, ap-east-1, cn-north-1, us-gov-west-1, ap-southeast-1, ap-southeast-2, us-iso-east-1, us-east-1, us-east-2, cn-northwest-1, us-isob-east-1, aws-global, aws-cn-global, aws-us-gov-global, aws-iso-global, aws-iso-b-global | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#ibm",
    "relUrl": "/user/sinks.html#ibm"
  },"117": {
    "doc": "Sinks",
    "title": "KAFKA",
    "content": "Kafka . Description: . Class: ca.breakpoints.datamaker.sink.kafka.KafkaOutputSink . Configuration: . | Kafka synchronous send . | Type: BOOLEAN | Default value: False | Possible values: | . | Kafka topic name . | Type: STRING | Default value: | Possible values: | . | Kafka bootstrap servers (host1:port1,host2:port2,...) . | Type: STRING | Default value: | Possible values: | . | Kafka client id . | Type: STRING | Default value: | Possible values: | . | Security protocols . | Type: STRING | Default value: PLAINTEXT | Possible values: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL | . | Kerberos keytab . | Type: STRING | Default value: | Possible values: | . | Kerberos principal . | Type: STRING | Default value: | Possible values: | . | Kafka header names . | Type: LIST | Default value: [] | Possible values: | . | Kafka header values (support expression) . | Type: LIST | Default value: [] | Possible values: | . | Truststore filename . | Type: STRING | Default value: | Possible values: | . | Truststore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | Keystore filename . | Type: STRING | Default value: | Possible values: | . | Keystore password . | Type: PASSWORD | Default value: changeit | Possible values: | . | . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#kafka",
    "relUrl": "/user/sinks.html#kafka"
  },"118": {
    "doc": "Sinks",
    "title": "SINK",
    "content": "Object . Description: . Class: ca.breakpoints.datamaker.sink.ObjectOutputSink . Configuration: . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#sink",
    "relUrl": "/user/sinks.html#sink"
  },"119": {
    "doc": "Sinks",
    "title": "SOLR",
    "content": "Solr . Description: . Class: ca.breakpoints.datamaker.sink.solr.SolrOutputSink . Configuration: . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/sinks.html#solr",
    "relUrl": "/user/sinks.html#solr"
  },"120": {
    "doc": "Contact",
    "title": "Contact links",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/contact/#contact-links",
    "relUrl": "/contact/#contact-links"
  },"121": {
    "doc": "Contact",
    "title": "Phone",
    "content": "Toll Free Number: 1-877-347-8141 . ",
    "url": "https://www.datamaker.ai/datamaker/docs/contact/#phone",
    "relUrl": "/contact/#phone"
  },"122": {
    "doc": "Contact",
    "title": "Chat",
    "content": "Direct . Click on the chat icon in the bottom right corner of every pages on the main website. Slack . Coming soon… . ",
    "url": "https://www.datamaker.ai/datamaker/docs/contact/#chat",
    "relUrl": "/contact/#chat"
  },"123": {
    "doc": "Contact",
    "title": "Email",
    "content": "General information: info@datamaker.ai . Sales: sales@datamaker.ai . Support: support@datamaker.ai . ",
    "url": "https://www.datamaker.ai/datamaker/docs/contact/#email",
    "relUrl": "/contact/#email"
  },"124": {
    "doc": "Contact",
    "title": "Support portal",
    "content": "www.datamaker.ai/support . ",
    "url": "https://www.datamaker.ai/datamaker/docs/contact/#support-portal",
    "relUrl": "/contact/#support-portal"
  },"125": {
    "doc": "Contact",
    "title": "Website",
    "content": "www.datamaker.ai . ",
    "url": "https://www.datamaker.ai/datamaker/docs/contact/#website",
    "relUrl": "/contact/#website"
  },"126": {
    "doc": "Contact",
    "title": "Contact",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/contact/",
    "relUrl": "/contact/"
  },"127": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting",
    "content": " ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html",
    "relUrl": "/admin/troubleshooting.html"
  },"128": {
    "doc": "Troubleshooting",
    "title": "Version info",
    "content": "Use this information in case you need to contact support. Logs viewer . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html#version-info",
    "relUrl": "/admin/troubleshooting.html#version-info"
  },"129": {
    "doc": "Troubleshooting",
    "title": "Verify logs",
    "content": "You can monitor the logs for error using from the log viewer page. The view is updated automatically. You can also download the logs and attach it to a support ticket. Logs viewer . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html#verify-logs",
    "relUrl": "/admin/troubleshooting.html#verify-logs"
  },"130": {
    "doc": "Troubleshooting",
    "title": "Performance indicators",
    "content": "You can profile the application from the health page. The metrics are updated continuously. Performance . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html#performance-indicators",
    "relUrl": "/admin/troubleshooting.html#performance-indicators"
  },"131": {
    "doc": "Troubleshooting",
    "title": "Rest Queries",
    "content": "More information here: Rest API . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html#rest-queries",
    "relUrl": "/admin/troubleshooting.html#rest-queries"
  },"132": {
    "doc": "Troubleshooting",
    "title": "Flush cache",
    "content": "curl /datamaker/api/system/clearAllCaches . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html#flush-cache",
    "relUrl": "/admin/troubleshooting.html#flush-cache"
  },"133": {
    "doc": "Troubleshooting",
    "title": "Change log level",
    "content": "Check current log level . curl /datamaker/actuator/loggers/org.hibernate.SQL {\"configuredLevel\":null,\"effectiveLevel\":\"INFO\"} . Change log level . curl -i -X POST -H 'Content-Type: application/json' -d '{\"configuredLevel\": \"DEBUG\"}' /datamaker/actuator/loggers/org.hibernate.SQL . Confirm change . curl /datamaker/actuator/loggers/org.hibernate.SQL {\"configuredLevel\":null,\"effectiveLevel\":\"DEBUG\"} . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html#change-log-level",
    "relUrl": "/admin/troubleshooting.html#change-log-level"
  },"134": {
    "doc": "Troubleshooting",
    "title": "Rebuild search index",
    "content": "curl /datamaker/api/system/search-index/rebuild . ",
    "url": "https://www.datamaker.ai/datamaker/docs/admin/troubleshooting.html#rebuild-search-index",
    "relUrl": "/admin/troubleshooting.html#rebuild-search-index"
  },"135": {
    "doc": "Users",
    "title": "Users",
    "content": "Use this page to create, list or manage the registered users. To view a list of all users: Users . To create a new user: Create User . | First name | Last name | Username | Password . | Policy: Must contain at least one number and one uppercase and lowercase letter, and at least 8 or more characters | . | Role: Specify the role | Language: Set the user language | Assign the groups | . To view all groups: Manage Groups . To create a group: Create Group . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/users.html",
    "relUrl": "/user/users.html"
  },"136": {
    "doc": "Users",
    "title": "To edit your profile: Profile",
    "content": "You can change your password by clicking of the account icon in the top right corner and select Profile. ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/users.html#to-edit-your-profile-profile",
    "relUrl": "/user/users.html#to-edit-your-profile-profile"
  },"137": {
    "doc": "Workspaces",
    "title": "Workspaces",
    "content": "The concept of a workspace is similar to a project. A workspace contains all related components (datasets, fields, sinks). It’s a way to segment data jobs logically. To view a list of all workspaces: Workspaces . From this view you can see who is the owner of the workspace and if any group is assigned to it. You can also edit or delete the workspace from this view. To create a new workspace: Create Workspace . Using this form you can create or edit workspace. By default, only the user can modify the workspace and no group is defined. | Name | Description | Owner: The default owner assigned is the current logged user. You can change it by specifying the username | Group: assign a group to the workspace | Group Permissions: . | NONE: disable access | READ_ONLY: browse only | READ_EXECUTE: browse and run jobs | READ_WRITE: create and modify items | FULL: admin privileges | . | . If you are an admin, you can create new groups form the Users section . ",
    "url": "https://www.datamaker.ai/datamaker/docs/user/workspaces.html",
    "relUrl": "/user/workspaces.html"
  }
}
